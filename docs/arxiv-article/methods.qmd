---
title: "Methods"
format: pdf
---

# Detailed Methodology

## Model Architectures

We systematically evaluate a family of convolutional neural network (CNN) architectures with varying depths, widths, and structural elements. Our exploration includes:

### Base Architecture

Our baseline architecture consists of:

$$
\begin{aligned}
\text{Input} &\rightarrow \text{Conv2D}(32, 3\times3, \text{ReLU}) \rightarrow \text{MaxPool}(2\times2) \\
&\rightarrow \text{Conv2D}(64, 3\times3, \text{ReLU}) \rightarrow \text{MaxPool}(2\times2) \\
&\rightarrow \text{Flatten} \rightarrow \text{Dense}(128, \text{ReLU}) \rightarrow \text{Dropout}(0.5) \\
&\rightarrow \text{Dense}(10, \text{softmax})
\end{aligned}
$$

### Architecture Variations

We systematically explore variations in:

1. **Network Depth**: Adding 1-3 additional convolutional layers
2. **Network Width**: Varying the number of filters (16, 32, 64, 128) and neurons in dense layers (64, 128, 256)
3. **Kernel Sizes**: Testing different filter sizes (3×3, 5×5, 7×7)
4. **Pooling Strategies**: Max pooling vs. average pooling

### Initialization Schemes

For each architecture, we evaluate three weight initialization strategies:

1. **Xavier/Glorot initialization** [@glorot2010understanding]: Weights are sampled from a uniform distribution with bounds $\pm \sqrt{6/(n_{\text{in}} + n_{\text{out}})}$
2. **He initialization**: Weights are sampled from a normal distribution with standard deviation $\sqrt{2/n_{\text{in}}}$
3. **Orthogonal initialization**: Weights are initialized as orthogonal matrices

## Optimization Methods

We compare the following optimization algorithms:

1. **Stochastic Gradient Descent (SGD)**: With and without momentum (0.9)
2. **RMSProp**: With decay factor 0.9
3. **Adam** [@kingma2014adam]: With $\beta_1 = 0.9$, $\beta_2 = 0.999$, and $\epsilon = 10^{-8}$

For each optimizer, we evaluate the following learning rate schedules:

1. **Constant**: Fixed learning rate throughout training
2. **Step decay**: Reducing the learning rate by a factor of 0.1 every 5 epochs
3. **Exponential decay**: $\alpha_t = \alpha_0 \cdot e^{-kt}$ where $k = 0.1$
4. **Cosine annealing**: $\alpha_t = \alpha_0 \cdot \frac{1}{2}(1 + \cos(\frac{t\pi}{T}))$

## Regularization Techniques

We evaluate the following regularization strategies:

1. **Weight decay (L2 regularization)**: With coefficients λ ∈ {0.0001, 0.001, 0.01}
2. **Dropout** [@srivastava2014dropout]: With rates p ∈ {0.2, 0.5, 0.7}
3. **Batch normalization** [@ioffe2015batch]: Applied before activation functions
4. **Data augmentation**: Random rotations (±10°), translations (±2 pixels), and zoom (±10%)

## Training Protocol

All models are trained with the following protocol:

1. **Batch size**: 128
2. **Epochs**: 20
3. **Validation split**: 20% of training data
4. **Early stopping**: With patience of 5 epochs and monitoring validation loss
5. **Model checkpointing**: Saving the best model based on validation accuracy

## Evaluation Metrics

We evaluate model performance using:

1. **Accuracy**: Percentage of correctly classified images
2. **Loss**: Cross-entropy loss
3. **Precision, Recall, F1-score**: For each digit class
4. **Confusion matrix**: To analyze patterns of misclassification
5. **ROC curves and AUC**: For each digit class

## Visualization Methods

To analyze model behavior, we employ the following visualization techniques:

1. **Learning curves**: Plotting training/validation loss and accuracy
2. **Weight visualizations**: Displaying learned filters in convolutional layers
3. **Activation maps**: Visualizing feature activations for sample inputs
4. **t-SNE projections** [@maaten2008visualizing]: Dimensionality reduction for hidden representations
5. **Saliency maps**: Highlighting regions of input images most influential for predictions

## Reproducibility Considerations

To ensure reproducibility of our results, we:

1. Set random seeds (42) for all random number generators
2. Use identical data splits across all experiments
3. Run each experiment configuration 5 times and report mean and standard deviation
4. Make our code and trained models publicly available

## Computational Resources

All experiments were conducted on:

- Hardware: NVIDIA RTX 3090 GPU with 24GB VRAM
- Software: PyTorch 1.9.0, CUDA 11.1
- Average training time: 2-5 minutes per model 