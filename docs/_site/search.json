[
  {
    "objectID": "model-analysis.html",
    "href": "model-analysis.html",
    "title": "MNIST Model Analysis",
    "section": "",
    "text": "In this document, we will analyze the performance and prediction results of trained MNIST classification models.\n\n\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nimport json\n\n# Set matplotlib style\nplt.style.use('ggplot')\n\n\n\n\nFirst, we need to define the same model structure used during training.\n\nclass Net(nn.Module):\n    \"\"\"CNN model for MNIST classification\"\"\"\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)  # First convolutional layer: 1 channel in, 32 out, 3x3 kernel\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)  # Second convolutional layer: 32 channels in, 64 out, 3x3 kernel\n        self.dropout1 = nn.Dropout(0.25)  # Dropout layer with 0.25 probability\n        self.dropout2 = nn.Dropout(0.5)   # Dropout layer with 0.5 probability\n        self.fc1 = nn.Linear(9216, 128)   # First fully connected layer\n        self.fc2 = nn.Linear(128, 10)     # Output layer: 10 classes for digits 0-9\n\n    def forward(self, x):\n        \"\"\"Forward pass through the network\"\"\"\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)  # Apply log softmax for NLL loss\n        return output\n\n\n\n\n\n# Define data transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# Load test dataset\ndata_dir = '../data'\ntest_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000)\n\n# Determine available device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \n                     \"mps\" if torch.backends.mps.is_available() else \n                     \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Try to load the model\ntry:\n    # Try multiple possible environments\n    env_types = [\"default\", \"cpu\", \"cuda\", \"mps\"]\n    model_loaded = False\n    \n    for env_type in env_types:\n        model_path = f\"../models/{env_type}/mnist_cnn.pt\"\n        if os.path.exists(model_path):\n            # Create model instance\n            model = Net().to(device)\n            # Load model weights\n            model.load_state_dict(torch.load(model_path, map_location=device))\n            model.eval()  # Set to evaluation mode\n            print(f\"✅ Successfully loaded model from {model_path}\")\n            model_loaded = True\n            break\n    \n    if not model_loaded:\n        raise FileNotFoundError(\"Could not find trained model file\")\n    \nexcept Exception as e:\n    print(f\"❌ Error loading model: {str(e)}\")\n    print(\"Please use 'pixi run train-model' to train the model first\")\n    # Create untrained model for demonstration\n    model = Net().to(device)\n    model.eval()\n\nUsing device: mps\n✅ Successfully loaded model from ../models/cpu/mnist_cnn.pt\n\n\n/var/folders/9b/nq3qtb3n0cxgw9m4sy1sycrh0000gn/T/ipykernel_36502/2626477224.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n\n\n\n\n\n\n# Try to load results file\ntry:\n    # Try multiple possible environments\n    env_types = [\"default\", \"cpu\", \"cuda\", \"mps\"]\n    results_loaded = False\n    \n    for env_type in env_types:\n        results_path = f\"../results/{env_type}/mnist_results.json\"\n        if os.path.exists(results_path):\n            # Load results data\n            with open(results_path, 'r') as f:\n                results = json.load(f)\n            print(f\"✅ Successfully loaded results data from {results_path}\")\n            results_loaded = True\n            break\n    \n    if not results_loaded:\n        raise FileNotFoundError(\"Could not find training results file\")\n    \nexcept Exception as e:\n    print(f\"❌ Error loading results: {str(e)}\")\n    print(\"Please use 'pixi run train-model' and 'pixi run test-model' to generate results\")\n    # Create empty results dictionary for demonstration\n    results = {\"training_history\": [], \"testing_history\": []}\n\n✅ Successfully loaded results data from ../results/cpu/mnist_results.json\n\n\n\n\n\n\n\nAnalyze the changes in loss and accuracy during the training process.\n\nif 'training_history' in results and len(results['training_history']) &gt; 0:\n    # Organize training history data\n    train_data = results['training_history']\n    \n    # Group by epoch to calculate average loss\n    epoch_losses = {}\n    for entry in train_data:\n        epoch = entry['epoch']\n        if epoch not in epoch_losses:\n            epoch_losses[epoch] = []\n        epoch_losses[epoch].append(entry['loss'])\n    \n    # Calculate average loss for each epoch\n    epochs = sorted(epoch_losses.keys())\n    avg_losses = [np.mean(epoch_losses[e]) for e in epochs]\n    \n    # Get test history\n    test_history = results.get('testing_history', [])\n    test_epochs = [entry['epoch'] for entry in test_history]\n    test_losses = [entry['loss'] for entry in test_history]\n    test_accuracies = [entry['accuracy'] for entry in test_history]\n    \n    # Create figure\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Training loss curve\n    ax1.plot(epochs, avg_losses, 'o-', color='blue', label='Training Loss')\n    if test_history:\n        ax1.plot(test_epochs, test_losses, 's-', color='red', label='Test Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('Training and Test Loss')\n    ax1.legend()\n    ax1.grid(True)\n    \n    # Test accuracy curve\n    if test_history:\n        ax2.plot(test_epochs, test_accuracies, 's-', color='green')\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('Accuracy (%)')\n        ax2.set_title('Test Accuracy')\n        ax2.grid(True)\n    else:\n        ax2.text(0.5, 0.5, 'No test accuracy data available', ha='center', va='center')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Display final test results\n    if 'final_test_result' in results:\n        final_result = results['final_test_result']\n        print(f\"Final test loss: {final_result['loss']:.4f}\")\n        print(f\"Final test accuracy: {final_result['accuracy']:.2f}%\")\n        print(f\"Correct predictions: {final_result['correct']}/{final_result['total']}\")\nelse:\n    print(\"No training history data available\")\n\n\n\n\nModel Training History\n\n\n\n\nFinal test loss: 0.0276\nFinal test accuracy: 99.11%\nCorrect predictions: 9911/10000\n\n\n\n\n\nCalculate and visualize the model’s confusion matrix on the test set.\n\ndef evaluate_model(model, data_loader, device):\n    model.eval()\n    all_preds = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for data, target in data_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            pred = output.argmax(dim=1, keepdim=True).squeeze()\n            all_preds.extend(pred.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n    \n    return np.array(all_preds), np.array(all_targets)\n\n# Evaluate model on test set\ntry:\n    predictions, targets = evaluate_model(model, test_loader, device)\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(targets, predictions)\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n                xticklabels=range(10), yticklabels=range(10))\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.title('Confusion Matrix')\n    plt.tight_layout()\n    plt.show()\n    \n    # Calculate and display classification report\n    report = classification_report(targets, predictions, output_dict=True)\n    report_df = pd.DataFrame(report).transpose()\n    \n    # Filter and rename columns\n    if 'accuracy' in report_df.index:\n        accuracy_row = report_df.loc[['accuracy']]\n        report_df = report_df.drop('accuracy')\n        report_df = report_df.drop('macro avg', errors='ignore')\n        report_df = report_df.drop('weighted avg', errors='ignore')\n    \n    # Display performance for each digit\n    print(\"Classification performance by digit:\")\n    print(report_df.round(3))\n    \nexcept Exception as e:\n    print(f\"Error evaluating model: {str(e)}\")\n\n\n\n\nConfusion Matrix on Test Set\n\n\n\n\nClassification performance by digit:\n   precision  recall  f1-score  support\n0      0.988   0.998     0.993    980.0\n1      0.996   0.998     0.997   1135.0\n2      0.990   0.991     0.991   1032.0\n3      0.995   0.992     0.994   1010.0\n4      0.995   0.990     0.992    982.0\n5      0.988   0.992     0.990    892.0\n6      0.995   0.983     0.989    958.0\n7      0.989   0.990     0.990   1028.0\n8      0.985   0.990     0.987    974.0\n9      0.989   0.985     0.987   1009.0\n\n\n\n\n\n\nView prediction results on some test samples.\n\ndef visualize_predictions(model, dataset, device, num_samples=25):\n    # Create data loader\n    loader = torch.utils.data.DataLoader(dataset, batch_size=num_samples)\n    \n    # Get a batch of data\n    data, targets = next(iter(loader))\n    data, targets = data.to(device), targets.to(device)\n    \n    # Get predictions\n    model.eval()\n    with torch.no_grad():\n        outputs = model(data)\n        probs = torch.exp(outputs)\n        preds = outputs.argmax(dim=1)\n    \n    # Move data back to CPU\n    images = data.cpu().numpy()\n    targets = targets.cpu().numpy()\n    preds = preds.cpu().numpy()\n    probs = probs.cpu().numpy()\n    \n    # Create figure\n    rows, cols = 5, 5\n    fig, axes = plt.subplots(rows, cols, figsize=(12, 12))\n    \n    for i, ax in enumerate(axes.flat):\n        if i &lt; num_samples:\n            # Display image\n            img = images[i][0]\n            ax.imshow(img, cmap='gray')\n            \n            # Set title\n            true_label = targets[i]\n            pred_label = preds[i]\n            probability = probs[i][pred_label]\n            \n            title = f\"True: {true_label}, Pred: {pred_label}\"\n            color = 'green' if true_label == pred_label else 'red'\n            \n            ax.set_title(title, color=color)\n            ax.text(0.5, -0.15, f\"Probability: {probability:.2f}\", \n                   transform=ax.transAxes, ha='center')\n            ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ntry:\n    # Visualize some predictions\n    visualize_predictions(model, test_dataset, device)\nexcept Exception as e:\n    print(f\"Error visualizing predictions: {str(e)}\")\n\n\n\n\nTest Sample Prediction Results\n\n\n\n\n\n\n\nAnalyze which samples the model tends to misclassify.\n\ndef analyze_errors(model, dataset, device, num_errors=15):\n    # Create data loader\n    loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n    \n    # Collect error predictions\n    errors = []\n    \n    model.eval()\n    with torch.no_grad():\n        for i, (data, target) in enumerate(loader):\n            if len(errors) &gt;= num_errors:\n                break\n                \n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            pred = output.argmax(dim=1)\n            \n            if pred.item() != target.item():\n                probs = torch.exp(output)\n                errors.append({\n                    'image': data.cpu().squeeze().numpy(),\n                    'true': target.item(),\n                    'pred': pred.item(),\n                    'probs': probs.cpu().numpy()[0],\n                    'index': i\n                })\n    \n    if not errors:\n        print(\"No error predictions found\")\n        return\n    \n    # Create figure\n    rows, cols = 3, 5\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 9))\n    \n    for i, ax in enumerate(axes.flat):\n        if i &lt; len(errors):\n            err = errors[i]\n            \n            # Display image\n            ax.imshow(err['image'], cmap='gray')\n            \n            # Set title\n            title = f\"True: {err['true']}, Pred: {err['pred']}\"\n            ax.set_title(title, color='red')\n            \n            # Display top 3 highest probabilities\n            top_k = np.argsort(err['probs'])[-3:][::-1]\n            probs_text = \"\\n\".join([f\"{j}: {err['probs'][j]:.2f}\" for j in top_k])\n            ax.text(0.95, 0.05, probs_text, \n                   transform=ax.transAxes, ha='right', va='bottom',\n                   bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n            \n            ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ntry:\n    # Analyze error predictions\n    analyze_errors(model, test_dataset, device)\nexcept Exception as e:\n    print(f\"Error analyzing errors: {str(e)}\")\n\n\n\n\nError Prediction Sample Analysis\n\n\n\n\n\n\n\nUse gradient visualization to analyze which pixels are most important for the model’s decision.\n\ndef compute_gradients(model, image, target, device):\n    # Add gradient tracking to image\n    image.requires_grad_()\n    \n    # Forward pass\n    model.eval()\n    output = model(image)\n    \n    # Calculate gradient for target class\n    model.zero_grad()\n    one_hot = torch.zeros_like(output)\n    one_hot[0, target] = 1\n    output.backward(gradient=one_hot)\n    \n    # Get gradient\n    return image.grad.data.abs().cpu().numpy()[0][0]\n\ntry:\n    # Select some samples from test set\n    samples = [0, 1000, 2000, 3000, 4000]\n    num_samples = len(samples)\n    \n    # Create figure\n    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n    \n    for i, sample_idx in enumerate(samples):\n        # Get sample\n        image, target = test_dataset[sample_idx]\n        image = image.unsqueeze(0).to(device)  # Add batch dimension\n        \n        # Make prediction\n        model.eval()\n        with torch.no_grad():\n            output = model(image)\n            pred = output.argmax(dim=1).item()\n        \n        # Calculate gradient\n        gradient = compute_gradients(model, image.clone(), target, device)\n        \n        # Display original image\n        axes[i, 0].imshow(image.squeeze().cpu().numpy(), cmap='gray')\n        axes[i, 0].set_title(f'Original (Label: {target})')\n        axes[i, 0].axis('off')\n        \n        # Display gradient\n        axes[i, 1].imshow(gradient, cmap='hot')\n        axes[i, 1].set_title('Feature Importance Heatmap')\n        axes[i, 1].axis('off')\n        \n        # Display overlay\n        img = image.squeeze().cpu().numpy()\n        overlay = np.zeros((28, 28, 3))\n        overlay[:, :, 0] = gradient / gradient.max()  # Red channel for gradient\n        overlay[:, :, 1] = img  # Green channel for original image\n        overlay[:, :, 2] = img  # Blue channel for original image\n        \n        axes[i, 2].imshow(overlay)\n        axes[i, 2].set_title('Overlay Visualization')\n        axes[i, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \nexcept Exception as e:\n    print(f\"Error computing feature importance: {str(e)}\")\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4242129623889923..2.821486711502075].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4242129623889923..2.821486711502075].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4242129623889923..2.821486711502075].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4242129623889923..2.821486711502075].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4242129623889923..2.821486711502075].\n\n\n\n\n\nModel Feature Importance Visualization\n\n\n\n\n\n\n\nFrom the above analysis, we can draw the following conclusions about the MNIST model:\n\nThe model achieves high classification accuracy, indicating that CNNs are very effective for handwritten digit recognition tasks\nDifferent digits have varying recognition difficulty, with some digits (like 1 and 0) being easier to recognize, while others (like 5 and 8) may be more challenging\nThe confusion matrix shows the most common confusion pairs (e.g., 4 and 9, or 3 and 5)\nFeature importance analysis indicates that the model primarily focuses on structural features of digits, such as strokes and intersections"
  },
  {
    "objectID": "model-analysis.html#loading-necessary-libraries",
    "href": "model-analysis.html#loading-necessary-libraries",
    "title": "MNIST Model Analysis",
    "section": "",
    "text": "import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nimport json\n\n# Set matplotlib style\nplt.style.use('ggplot')"
  },
  {
    "objectID": "model-analysis.html#define-model-structure",
    "href": "model-analysis.html#define-model-structure",
    "title": "MNIST Model Analysis",
    "section": "",
    "text": "First, we need to define the same model structure used during training.\n\nclass Net(nn.Module):\n    \"\"\"CNN model for MNIST classification\"\"\"\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)  # First convolutional layer: 1 channel in, 32 out, 3x3 kernel\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)  # Second convolutional layer: 32 channels in, 64 out, 3x3 kernel\n        self.dropout1 = nn.Dropout(0.25)  # Dropout layer with 0.25 probability\n        self.dropout2 = nn.Dropout(0.5)   # Dropout layer with 0.5 probability\n        self.fc1 = nn.Linear(9216, 128)   # First fully connected layer\n        self.fc2 = nn.Linear(128, 10)     # Output layer: 10 classes for digits 0-9\n\n    def forward(self, x):\n        \"\"\"Forward pass through the network\"\"\"\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)  # Apply log softmax for NLL loss\n        return output"
  },
  {
    "objectID": "model-analysis.html#loading-data-and-model",
    "href": "model-analysis.html#loading-data-and-model",
    "title": "MNIST Model Analysis",
    "section": "",
    "text": "# Define data transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# Load test dataset\ndata_dir = '../data'\ntest_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000)\n\n# Determine available device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \n                     \"mps\" if torch.backends.mps.is_available() else \n                     \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Try to load the model\ntry:\n    # Try multiple possible environments\n    env_types = [\"default\", \"cpu\", \"cuda\", \"mps\"]\n    model_loaded = False\n    \n    for env_type in env_types:\n        model_path = f\"../models/{env_type}/mnist_cnn.pt\"\n        if os.path.exists(model_path):\n            # Create model instance\n            model = Net().to(device)\n            # Load model weights\n            model.load_state_dict(torch.load(model_path, map_location=device))\n            model.eval()  # Set to evaluation mode\n            print(f\"✅ Successfully loaded model from {model_path}\")\n            model_loaded = True\n            break\n    \n    if not model_loaded:\n        raise FileNotFoundError(\"Could not find trained model file\")\n    \nexcept Exception as e:\n    print(f\"❌ Error loading model: {str(e)}\")\n    print(\"Please use 'pixi run train-model' to train the model first\")\n    # Create untrained model for demonstration\n    model = Net().to(device)\n    model.eval()\n\nUsing device: mps\n✅ Successfully loaded model from ../models/cpu/mnist_cnn.pt\n\n\n/var/folders/9b/nq3qtb3n0cxgw9m4sy1sycrh0000gn/T/ipykernel_36502/2626477224.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))"
  },
  {
    "objectID": "model-analysis.html#loading-training-results",
    "href": "model-analysis.html#loading-training-results",
    "title": "MNIST Model Analysis",
    "section": "",
    "text": "# Try to load results file\ntry:\n    # Try multiple possible environments\n    env_types = [\"default\", \"cpu\", \"cuda\", \"mps\"]\n    results_loaded = False\n    \n    for env_type in env_types:\n        results_path = f\"../results/{env_type}/mnist_results.json\"\n        if os.path.exists(results_path):\n            # Load results data\n            with open(results_path, 'r') as f:\n                results = json.load(f)\n            print(f\"✅ Successfully loaded results data from {results_path}\")\n            results_loaded = True\n            break\n    \n    if not results_loaded:\n        raise FileNotFoundError(\"Could not find training results file\")\n    \nexcept Exception as e:\n    print(f\"❌ Error loading results: {str(e)}\")\n    print(\"Please use 'pixi run train-model' and 'pixi run test-model' to generate results\")\n    # Create empty results dictionary for demonstration\n    results = {\"training_history\": [], \"testing_history\": []}\n\n✅ Successfully loaded results data from ../results/cpu/mnist_results.json"
  },
  {
    "objectID": "model-analysis.html#model-performance-analysis",
    "href": "model-analysis.html#model-performance-analysis",
    "title": "MNIST Model Analysis",
    "section": "",
    "text": "Analyze the changes in loss and accuracy during the training process.\n\nif 'training_history' in results and len(results['training_history']) &gt; 0:\n    # Organize training history data\n    train_data = results['training_history']\n    \n    # Group by epoch to calculate average loss\n    epoch_losses = {}\n    for entry in train_data:\n        epoch = entry['epoch']\n        if epoch not in epoch_losses:\n            epoch_losses[epoch] = []\n        epoch_losses[epoch].append(entry['loss'])\n    \n    # Calculate average loss for each epoch\n    epochs = sorted(epoch_losses.keys())\n    avg_losses = [np.mean(epoch_losses[e]) for e in epochs]\n    \n    # Get test history\n    test_history = results.get('testing_history', [])\n    test_epochs = [entry['epoch'] for entry in test_history]\n    test_losses = [entry['loss'] for entry in test_history]\n    test_accuracies = [entry['accuracy'] for entry in test_history]\n    \n    # Create figure\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Training loss curve\n    ax1.plot(epochs, avg_losses, 'o-', color='blue', label='Training Loss')\n    if test_history:\n        ax1.plot(test_epochs, test_losses, 's-', color='red', label='Test Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('Training and Test Loss')\n    ax1.legend()\n    ax1.grid(True)\n    \n    # Test accuracy curve\n    if test_history:\n        ax2.plot(test_epochs, test_accuracies, 's-', color='green')\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('Accuracy (%)')\n        ax2.set_title('Test Accuracy')\n        ax2.grid(True)\n    else:\n        ax2.text(0.5, 0.5, 'No test accuracy data available', ha='center', va='center')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Display final test results\n    if 'final_test_result' in results:\n        final_result = results['final_test_result']\n        print(f\"Final test loss: {final_result['loss']:.4f}\")\n        print(f\"Final test accuracy: {final_result['accuracy']:.2f}%\")\n        print(f\"Correct predictions: {final_result['correct']}/{final_result['total']}\")\nelse:\n    print(\"No training history data available\")\n\n\n\n\nModel Training History\n\n\n\n\nFinal test loss: 0.0276\nFinal test accuracy: 99.11%\nCorrect predictions: 9911/10000\n\n\n\n\n\nCalculate and visualize the model’s confusion matrix on the test set.\n\ndef evaluate_model(model, data_loader, device):\n    model.eval()\n    all_preds = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for data, target in data_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            pred = output.argmax(dim=1, keepdim=True).squeeze()\n            all_preds.extend(pred.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n    \n    return np.array(all_preds), np.array(all_targets)\n\n# Evaluate model on test set\ntry:\n    predictions, targets = evaluate_model(model, test_loader, device)\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(targets, predictions)\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n                xticklabels=range(10), yticklabels=range(10))\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.title('Confusion Matrix')\n    plt.tight_layout()\n    plt.show()\n    \n    # Calculate and display classification report\n    report = classification_report(targets, predictions, output_dict=True)\n    report_df = pd.DataFrame(report).transpose()\n    \n    # Filter and rename columns\n    if 'accuracy' in report_df.index:\n        accuracy_row = report_df.loc[['accuracy']]\n        report_df = report_df.drop('accuracy')\n        report_df = report_df.drop('macro avg', errors='ignore')\n        report_df = report_df.drop('weighted avg', errors='ignore')\n    \n    # Display performance for each digit\n    print(\"Classification performance by digit:\")\n    print(report_df.round(3))\n    \nexcept Exception as e:\n    print(f\"Error evaluating model: {str(e)}\")\n\n\n\n\nConfusion Matrix on Test Set\n\n\n\n\nClassification performance by digit:\n   precision  recall  f1-score  support\n0      0.988   0.998     0.993    980.0\n1      0.996   0.998     0.997   1135.0\n2      0.990   0.991     0.991   1032.0\n3      0.995   0.992     0.994   1010.0\n4      0.995   0.990     0.992    982.0\n5      0.988   0.992     0.990    892.0\n6      0.995   0.983     0.989    958.0\n7      0.989   0.990     0.990   1028.0\n8      0.985   0.990     0.987    974.0\n9      0.989   0.985     0.987   1009.0"
  },
  {
    "objectID": "model-analysis.html#prediction-visualization",
    "href": "model-analysis.html#prediction-visualization",
    "title": "MNIST Model Analysis",
    "section": "",
    "text": "View prediction results on some test samples.\n\ndef visualize_predictions(model, dataset, device, num_samples=25):\n    # Create data loader\n    loader = torch.utils.data.DataLoader(dataset, batch_size=num_samples)\n    \n    # Get a batch of data\n    data, targets = next(iter(loader))\n    data, targets = data.to(device), targets.to(device)\n    \n    # Get predictions\n    model.eval()\n    with torch.no_grad():\n        outputs = model(data)\n        probs = torch.exp(outputs)\n        preds = outputs.argmax(dim=1)\n    \n    # Move data back to CPU\n    images = data.cpu().numpy()\n    targets = targets.cpu().numpy()\n    preds = preds.cpu().numpy()\n    probs = probs.cpu().numpy()\n    \n    # Create figure\n    rows, cols = 5, 5\n    fig, axes = plt.subplots(rows, cols, figsize=(12, 12))\n    \n    for i, ax in enumerate(axes.flat):\n        if i &lt; num_samples:\n            # Display image\n            img = images[i][0]\n            ax.imshow(img, cmap='gray')\n            \n            # Set title\n            true_label = targets[i]\n            pred_label = preds[i]\n            probability = probs[i][pred_label]\n            \n            title = f\"True: {true_label}, Pred: {pred_label}\"\n            color = 'green' if true_label == pred_label else 'red'\n            \n            ax.set_title(title, color=color)\n            ax.text(0.5, -0.15, f\"Probability: {probability:.2f}\", \n                   transform=ax.transAxes, ha='center')\n            ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ntry:\n    # Visualize some predictions\n    visualize_predictions(model, test_dataset, device)\nexcept Exception as e:\n    print(f\"Error visualizing predictions: {str(e)}\")\n\n\n\n\nTest Sample Prediction Results"
  },
  {
    "objectID": "model-analysis.html#error-analysis",
    "href": "model-analysis.html#error-analysis",
    "title": "MNIST Model Analysis",
    "section": "",
    "text": "Analyze which samples the model tends to misclassify.\n\ndef analyze_errors(model, dataset, device, num_errors=15):\n    # Create data loader\n    loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n    \n    # Collect error predictions\n    errors = []\n    \n    model.eval()\n    with torch.no_grad():\n        for i, (data, target) in enumerate(loader):\n            if len(errors) &gt;= num_errors:\n                break\n                \n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            pred = output.argmax(dim=1)\n            \n            if pred.item() != target.item():\n                probs = torch.exp(output)\n                errors.append({\n                    'image': data.cpu().squeeze().numpy(),\n                    'true': target.item(),\n                    'pred': pred.item(),\n                    'probs': probs.cpu().numpy()[0],\n                    'index': i\n                })\n    \n    if not errors:\n        print(\"No error predictions found\")\n        return\n    \n    # Create figure\n    rows, cols = 3, 5\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 9))\n    \n    for i, ax in enumerate(axes.flat):\n        if i &lt; len(errors):\n            err = errors[i]\n            \n            # Display image\n            ax.imshow(err['image'], cmap='gray')\n            \n            # Set title\n            title = f\"True: {err['true']}, Pred: {err['pred']}\"\n            ax.set_title(title, color='red')\n            \n            # Display top 3 highest probabilities\n            top_k = np.argsort(err['probs'])[-3:][::-1]\n            probs_text = \"\\n\".join([f\"{j}: {err['probs'][j]:.2f}\" for j in top_k])\n            ax.text(0.95, 0.05, probs_text, \n                   transform=ax.transAxes, ha='right', va='bottom',\n                   bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n            \n            ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ntry:\n    # Analyze error predictions\n    analyze_errors(model, test_dataset, device)\nexcept Exception as e:\n    print(f\"Error analyzing errors: {str(e)}\")\n\n\n\n\nError Prediction Sample Analysis"
  },
  {
    "objectID": "model-analysis.html#feature-importance-analysis",
    "href": "model-analysis.html#feature-importance-analysis",
    "title": "MNIST Model Analysis",
    "section": "",
    "text": "Use gradient visualization to analyze which pixels are most important for the model’s decision.\n\ndef compute_gradients(model, image, target, device):\n    # Add gradient tracking to image\n    image.requires_grad_()\n    \n    # Forward pass\n    model.eval()\n    output = model(image)\n    \n    # Calculate gradient for target class\n    model.zero_grad()\n    one_hot = torch.zeros_like(output)\n    one_hot[0, target] = 1\n    output.backward(gradient=one_hot)\n    \n    # Get gradient\n    return image.grad.data.abs().cpu().numpy()[0][0]\n\ntry:\n    # Select some samples from test set\n    samples = [0, 1000, 2000, 3000, 4000]\n    num_samples = len(samples)\n    \n    # Create figure\n    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n    \n    for i, sample_idx in enumerate(samples):\n        # Get sample\n        image, target = test_dataset[sample_idx]\n        image = image.unsqueeze(0).to(device)  # Add batch dimension\n        \n        # Make prediction\n        model.eval()\n        with torch.no_grad():\n            output = model(image)\n            pred = output.argmax(dim=1).item()\n        \n        # Calculate gradient\n        gradient = compute_gradients(model, image.clone(), target, device)\n        \n        # Display original image\n        axes[i, 0].imshow(image.squeeze().cpu().numpy(), cmap='gray')\n        axes[i, 0].set_title(f'Original (Label: {target})')\n        axes[i, 0].axis('off')\n        \n        # Display gradient\n        axes[i, 1].imshow(gradient, cmap='hot')\n        axes[i, 1].set_title('Feature Importance Heatmap')\n        axes[i, 1].axis('off')\n        \n        # Display overlay\n        img = image.squeeze().cpu().numpy()\n        overlay = np.zeros((28, 28, 3))\n        overlay[:, :, 0] = gradient / gradient.max()  # Red channel for gradient\n        overlay[:, :, 1] = img  # Green channel for original image\n        overlay[:, :, 2] = img  # Blue channel for original image\n        \n        axes[i, 2].imshow(overlay)\n        axes[i, 2].set_title('Overlay Visualization')\n        axes[i, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \nexcept Exception as e:\n    print(f\"Error computing feature importance: {str(e)}\")\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4242129623889923..2.821486711502075].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4242129623889923..2.821486711502075].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4242129623889923..2.821486711502075].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4242129623889923..2.821486711502075].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4242129623889923..2.821486711502075].\n\n\n\n\n\nModel Feature Importance Visualization"
  },
  {
    "objectID": "model-analysis.html#summary",
    "href": "model-analysis.html#summary",
    "title": "MNIST Model Analysis",
    "section": "",
    "text": "From the above analysis, we can draw the following conclusions about the MNIST model:\n\nThe model achieves high classification accuracy, indicating that CNNs are very effective for handwritten digit recognition tasks\nDifferent digits have varying recognition difficulty, with some digits (like 1 and 0) being easier to recognize, while others (like 5 and 8) may be more challenging\nThe confusion matrix shows the most common confusion pairs (e.g., 4 and 9, or 3 and 5)\nFeature importance analysis indicates that the model primarily focuses on structural features of digits, such as strokes and intersections"
  },
  {
    "objectID": "data-exploration.html",
    "href": "data-exploration.html",
    "title": "MNIST Dataset Exploration",
    "section": "",
    "text": "This document explores the characteristics and statistics of the MNIST dataset. We will load the data and perform visual analysis.\n\n\nFirst, we need to import the necessary libraries and load the MNIST dataset.\n\nimport sys\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torchvision import datasets, transforms\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# Set matplotlib style\nplt.style.use('ggplot')\n\n\n# Define data transformation\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# Load MNIST dataset\ndata_dir = '../data'\ntrain_dataset = datasets.MNIST(data_dir, train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\n\nprint(f\"Training set size: {len(train_dataset)} samples\")\nprint(f\"Test set size: {len(test_dataset)} samples\")\n\nTraining set size: 60000 samples\nTest set size: 10000 samples\n\n\n\n\n\n\n\nLet’s visualize some MNIST image samples to understand the characteristics of the data.\n\n# Create a function to display images\ndef show_images(dataset, num_images=25, rows=5, cols=5):\n    # Create a new figure\n    plt.figure(figsize=(12, 12))\n    \n    # Randomly select images\n    indices = np.random.choice(len(dataset), num_images, replace=False)\n    \n    # Display images\n    for i, idx in enumerate(indices):\n        if i &gt;= num_images:\n            break\n        \n        # Get image and label\n        img, label = dataset[idx]\n        img = img.squeeze().numpy()  # Convert to numpy and remove channel dimension\n        \n        # Create subplot\n        plt.subplot(rows, cols, i + 1)\n        plt.imshow(img, cmap='gray')\n        plt.title(f'Label: {label}')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Display some images from the training set\nshow_images(train_dataset)\n\n\n\n\nMNIST Dataset Sample Images\n\n\n\n\n\n\n\nLet’s look at the distribution of digits in the training and test sets.\n\n# Get all labels\ntrain_labels = [label for _, label in train_dataset]\ntest_labels = [label for _, label in test_dataset]\n\n# Calculate frequency of each digit\ndef plot_label_distribution(train_labels, test_labels):\n    train_counts = np.bincount(train_labels)\n    test_counts = np.bincount(test_labels)\n    \n    # Create dataframe\n    df = pd.DataFrame({\n        'Training Set': train_counts,\n        'Test Set': test_counts\n    }, index=range(10))\n    \n    # Create stacked bar chart\n    ax = df.plot(kind='bar', figsize=(12, 6), rot=0)\n    plt.title('Number of Samples for Each Digit in MNIST Dataset')\n    plt.xlabel('Digit')\n    plt.ylabel('Number of Samples')\n    plt.xticks(range(10), [str(i) for i in range(10)])\n    \n    # Add value labels to each bar\n    for container in ax.containers:\n        ax.bar_label(container, fmt='%d')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Return percentage distribution\n    train_pct = (train_counts / len(train_labels) * 100).round(2)\n    test_pct = (test_counts / len(test_labels) * 100).round(2)\n    \n    pct_df = pd.DataFrame({\n        'Training Set (%)': train_pct,\n        'Test Set (%)': test_pct\n    }, index=range(10))\n    \n    return pct_df\n\n# Plot label distribution and display percentage table\npct_table = plot_label_distribution(train_labels, test_labels)\npct_table\n\n\n\n\nLabel Distribution in Training and Test Sets\n\n\n\n\n\n\n\n\n\n\n\nTraining Set (%)\nTest Set (%)\n\n\n\n\n0\n9.87\n9.80\n\n\n1\n11.24\n11.35\n\n\n2\n9.93\n10.32\n\n\n3\n10.22\n10.10\n\n\n4\n9.74\n9.82\n\n\n5\n9.04\n8.92\n\n\n6\n9.86\n9.58\n\n\n7\n10.44\n10.28\n\n\n8\n9.75\n9.74\n\n\n9\n9.92\n10.09\n\n\n\n\n\n\n\n\n\n\nLet’s analyze the pixel intensity distribution of MNIST images.\n\n# Create a function to calculate pixel intensity statistics\ndef analyze_pixel_intensity(dataset, num_samples=1000):\n    # Randomly select samples\n    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n    \n    # Collect images\n    images = []\n    for idx in indices:\n        img, _ = dataset[idx]\n        # Remove normalization to get original pixel values\n        img = img * 0.3081 + 0.1307  # De-normalize\n        images.append(img.squeeze().numpy())\n    \n    # Stack images into a large array\n    images_array = np.stack(images)\n    \n    # Calculate average pixel intensity for each image\n    mean_intensities = images_array.mean(axis=(1, 2))\n    \n    # Calculate overall mean intensity\n    overall_mean = images_array.mean()\n    overall_std = images_array.std()\n    \n    # Create histograms\n    plt.figure(figsize=(12, 6))\n    \n    # Mean intensity histogram\n    plt.subplot(1, 2, 1)\n    plt.hist(mean_intensities, bins=30, alpha=0.7, color='blue')\n    plt.axvline(overall_mean, color='red', linestyle='dashed', linewidth=2)\n    plt.title(f'Mean Pixel Intensity Distribution\\nMean: {overall_mean:.4f}')\n    plt.xlabel('Mean Pixel Intensity')\n    plt.ylabel('Number of Images')\n    \n    # All pixel intensities histogram\n    plt.subplot(1, 2, 2)\n    plt.hist(images_array.flatten(), bins=50, alpha=0.7, color='green')\n    plt.title(f'All Pixel Intensity Distribution\\nStandard Deviation: {overall_std:.4f}')\n    plt.xlabel('Pixel Intensity')\n    plt.ylabel('Frequency')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return overall_mean, overall_std\n\n# Analyze pixel intensity of training set\nmean_intensity, std_intensity = analyze_pixel_intensity(train_dataset)\nprint(f\"Mean pixel intensity: {mean_intensity:.4f}\")\nprint(f\"Pixel intensity standard deviation: {std_intensity:.4f}\")\n\n\n\n\nMNIST Image Pixel Intensity Distribution\n\n\n\n\nMean pixel intensity: 0.1312\nPixel intensity standard deviation: 0.3089\n\n\n\n\n\nUsing PCA and t-SNE to visualize MNIST data distribution in lower-dimensional space.\n\n# Dimensionality reduction visualization function\ndef visualize_with_dimensionality_reduction(dataset, n_samples=2000):\n    # Randomly select samples\n    indices = np.random.choice(len(dataset), min(n_samples, len(dataset)), replace=False)\n    \n    # Collect data and labels\n    data = []\n    labels = []\n    for idx in indices:\n        img, label = dataset[idx]\n        data.append(img.squeeze().numpy().flatten())  # Flatten 28x28 to 784-dimensional vector\n        labels.append(label)\n    \n    # Convert to numpy arrays\n    X = np.array(data)\n    y = np.array(labels)\n    \n    # Use PCA to reduce to 2 dimensions\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n    \n    # Use t-SNE to reduce to 2 dimensions\n    tsne = TSNE(n_components=2, random_state=42)\n    X_tsne = tsne.fit_transform(X)\n    \n    # Create figure\n    plt.figure(figsize=(16, 7))\n    \n    # PCA scatter plot\n    plt.subplot(1, 2, 1)\n    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', s=5, alpha=0.7)\n    plt.colorbar(scatter, label='Digit Label')\n    plt.title('PCA Dimensionality Reduction (2D)')\n    plt.xlabel(f'Principal Component 1 (Explained Variance: {pca.explained_variance_ratio_[0]:.4f})')\n    plt.ylabel(f'Principal Component 2 (Explained Variance: {pca.explained_variance_ratio_[1]:.4f})')\n    \n    # t-SNE scatter plot\n    plt.subplot(1, 2, 2)\n    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=5, alpha=0.7)\n    plt.colorbar(scatter, label='Digit Label')\n    plt.title('t-SNE Dimensionality Reduction (2D)')\n    plt.xlabel('t-SNE Dimension 1')\n    plt.ylabel('t-SNE Dimension 2')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Return PCA variance explanation ratio\n    return pca.explained_variance_ratio_\n\n# Perform dimensionality reduction visualization on training set\nvar_ratio = visualize_with_dimensionality_reduction(train_dataset)\nprint(f\"Cumulative explained variance ratio of first 10 principal components in PCA: {np.sum(var_ratio):.4f}\")\n\n\n\n\nMNIST Data Visualization through PCA and t-SNE Dimensionality Reduction\n\n\n\n\nCumulative explained variance ratio of first 10 principal components in PCA: 0.1728\n\n\n\n\n\nDifferent digits have variations in shape and strokes. Let’s analyze the average shape and variability of each digit.\n\n# Analyze average shape and variance of each digit\ndef analyze_digit_features(dataset):\n    # Create a dictionary to store all images for each digit\n    digit_images = {i: [] for i in range(10)}\n    \n    # Collect images for each digit\n    for idx in range(len(dataset)):\n        img, label = dataset[idx]\n        img = img.squeeze().numpy()\n        digit_images[label].append(img)\n    \n    # Calculate average image and variance for each digit\n    mean_images = {}\n    var_images = {}\n    for digit, images in digit_images.items():\n        images_array = np.stack(images)\n        mean_images[digit] = np.mean(images_array, axis=0)\n        var_images[digit] = np.var(images_array, axis=0)\n    \n    # Create figure to display average image and variance for each digit\n    plt.figure(figsize=(15, 6))\n    \n    # Display average images\n    plt.subplot(1, 2, 1)\n    # Create a 3x4 grid to display all digits\n    grid_img = np.zeros((28*3, 28*4))\n    \n    for i, digit in enumerate(range(10)):\n        row, col = i // 4, i % 4\n        grid_img[row*28:(row+1)*28, col*28:(col+1)*28] = mean_images[digit]\n    \n    plt.imshow(grid_img, cmap='viridis')\n    plt.title('Average Shape of Each Digit')\n    plt.axis('off')\n    \n    # Display variance images\n    plt.subplot(1, 2, 2)\n    # Create a 3x4 grid to display variance for all digits\n    grid_var = np.zeros((28*3, 28*4))\n    \n    for i, digit in enumerate(range(10)):\n        row, col = i // 4, i % 4\n        grid_var[row*28:(row+1)*28, col*28:(col+1)*28] = var_images[digit]\n    \n    plt.imshow(grid_var, cmap='plasma')\n    plt.title('Pixel Variance of Each Digit')\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return mean_images, var_images\n\n# Analyze digit features\nmean_imgs, var_imgs = analyze_digit_features(train_dataset)\n\n\n\n\nAverage Shape and Variance of Each Digit\n\n\n\n\n\n\n\n\nAnalyze similarities and differences between different digits.\n\n# Calculate similarity between digits\ndef compute_digit_similarities(mean_images):\n    # Calculate 10x10 similarity matrix\n    similarity_matrix = np.zeros((10, 10))\n    \n    # Flatten average images\n    flattened_means = {digit: img.flatten() for digit, img in mean_images.items()}\n    \n    # Calculate correlation coefficient between each pair of digits\n    for i in range(10):\n        for j in range(10):\n            similarity_matrix[i, j] = np.corrcoef(flattened_means[i], flattened_means[j])[0, 1]\n    \n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    plt.imshow(similarity_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n    plt.colorbar(label='Correlation Coefficient')\n    plt.title('Similarity Matrix Between Digits')\n    plt.xlabel('Digit')\n    plt.ylabel('Digit')\n    plt.xticks(range(10))\n    plt.yticks(range(10))\n    \n    # Add correlation coefficient text\n    for i in range(10):\n        for j in range(10):\n            plt.text(j, i, f'{similarity_matrix[i, j]:.2f}', \n                    ha='center', va='center', \n                    color='white' if abs(similarity_matrix[i, j]) &gt; 0.5 else 'black')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return similarity_matrix\n\n# Calculate digit similarity\nsim_matrix = compute_digit_similarities(mean_imgs)\n\n\n\n\nSimilarity Matrix Between Digits\n\n\n\n\n\n\n\nFrom the above analysis, we can draw the following conclusions about the MNIST dataset:\n\nThe MNIST dataset is well-balanced, with similar distributions of digits in both training and test sets\nThe pixel intensity distribution shows the characteristics of handwritten digits, with most pixels being background (low intensity)\nDimensionality reduction analysis indicates that different digits form distinct clusters in feature space\nSome digits (such as 1 and 7) have higher similarity, while others (such as 0 and 1) have greater differences\nThe average images clearly show the typical shape of each digit"
  },
  {
    "objectID": "data-exploration.html#loading-data",
    "href": "data-exploration.html#loading-data",
    "title": "MNIST Dataset Exploration",
    "section": "",
    "text": "First, we need to import the necessary libraries and load the MNIST dataset.\n\nimport sys\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torchvision import datasets, transforms\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# Set matplotlib style\nplt.style.use('ggplot')\n\n\n# Define data transformation\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# Load MNIST dataset\ndata_dir = '../data'\ntrain_dataset = datasets.MNIST(data_dir, train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\n\nprint(f\"Training set size: {len(train_dataset)} samples\")\nprint(f\"Test set size: {len(test_dataset)} samples\")\n\nTraining set size: 60000 samples\nTest set size: 10000 samples"
  },
  {
    "objectID": "data-exploration.html#data-visualization",
    "href": "data-exploration.html#data-visualization",
    "title": "MNIST Dataset Exploration",
    "section": "",
    "text": "Let’s visualize some MNIST image samples to understand the characteristics of the data.\n\n# Create a function to display images\ndef show_images(dataset, num_images=25, rows=5, cols=5):\n    # Create a new figure\n    plt.figure(figsize=(12, 12))\n    \n    # Randomly select images\n    indices = np.random.choice(len(dataset), num_images, replace=False)\n    \n    # Display images\n    for i, idx in enumerate(indices):\n        if i &gt;= num_images:\n            break\n        \n        # Get image and label\n        img, label = dataset[idx]\n        img = img.squeeze().numpy()  # Convert to numpy and remove channel dimension\n        \n        # Create subplot\n        plt.subplot(rows, cols, i + 1)\n        plt.imshow(img, cmap='gray')\n        plt.title(f'Label: {label}')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Display some images from the training set\nshow_images(train_dataset)\n\n\n\n\nMNIST Dataset Sample Images\n\n\n\n\n\n\n\nLet’s look at the distribution of digits in the training and test sets.\n\n# Get all labels\ntrain_labels = [label for _, label in train_dataset]\ntest_labels = [label for _, label in test_dataset]\n\n# Calculate frequency of each digit\ndef plot_label_distribution(train_labels, test_labels):\n    train_counts = np.bincount(train_labels)\n    test_counts = np.bincount(test_labels)\n    \n    # Create dataframe\n    df = pd.DataFrame({\n        'Training Set': train_counts,\n        'Test Set': test_counts\n    }, index=range(10))\n    \n    # Create stacked bar chart\n    ax = df.plot(kind='bar', figsize=(12, 6), rot=0)\n    plt.title('Number of Samples for Each Digit in MNIST Dataset')\n    plt.xlabel('Digit')\n    plt.ylabel('Number of Samples')\n    plt.xticks(range(10), [str(i) for i in range(10)])\n    \n    # Add value labels to each bar\n    for container in ax.containers:\n        ax.bar_label(container, fmt='%d')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Return percentage distribution\n    train_pct = (train_counts / len(train_labels) * 100).round(2)\n    test_pct = (test_counts / len(test_labels) * 100).round(2)\n    \n    pct_df = pd.DataFrame({\n        'Training Set (%)': train_pct,\n        'Test Set (%)': test_pct\n    }, index=range(10))\n    \n    return pct_df\n\n# Plot label distribution and display percentage table\npct_table = plot_label_distribution(train_labels, test_labels)\npct_table\n\n\n\n\nLabel Distribution in Training and Test Sets\n\n\n\n\n\n\n\n\n\n\n\nTraining Set (%)\nTest Set (%)\n\n\n\n\n0\n9.87\n9.80\n\n\n1\n11.24\n11.35\n\n\n2\n9.93\n10.32\n\n\n3\n10.22\n10.10\n\n\n4\n9.74\n9.82\n\n\n5\n9.04\n8.92\n\n\n6\n9.86\n9.58\n\n\n7\n10.44\n10.28\n\n\n8\n9.75\n9.74\n\n\n9\n9.92\n10.09\n\n\n\n\n\n\n\n\n\n\nLet’s analyze the pixel intensity distribution of MNIST images.\n\n# Create a function to calculate pixel intensity statistics\ndef analyze_pixel_intensity(dataset, num_samples=1000):\n    # Randomly select samples\n    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n    \n    # Collect images\n    images = []\n    for idx in indices:\n        img, _ = dataset[idx]\n        # Remove normalization to get original pixel values\n        img = img * 0.3081 + 0.1307  # De-normalize\n        images.append(img.squeeze().numpy())\n    \n    # Stack images into a large array\n    images_array = np.stack(images)\n    \n    # Calculate average pixel intensity for each image\n    mean_intensities = images_array.mean(axis=(1, 2))\n    \n    # Calculate overall mean intensity\n    overall_mean = images_array.mean()\n    overall_std = images_array.std()\n    \n    # Create histograms\n    plt.figure(figsize=(12, 6))\n    \n    # Mean intensity histogram\n    plt.subplot(1, 2, 1)\n    plt.hist(mean_intensities, bins=30, alpha=0.7, color='blue')\n    plt.axvline(overall_mean, color='red', linestyle='dashed', linewidth=2)\n    plt.title(f'Mean Pixel Intensity Distribution\\nMean: {overall_mean:.4f}')\n    plt.xlabel('Mean Pixel Intensity')\n    plt.ylabel('Number of Images')\n    \n    # All pixel intensities histogram\n    plt.subplot(1, 2, 2)\n    plt.hist(images_array.flatten(), bins=50, alpha=0.7, color='green')\n    plt.title(f'All Pixel Intensity Distribution\\nStandard Deviation: {overall_std:.4f}')\n    plt.xlabel('Pixel Intensity')\n    plt.ylabel('Frequency')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return overall_mean, overall_std\n\n# Analyze pixel intensity of training set\nmean_intensity, std_intensity = analyze_pixel_intensity(train_dataset)\nprint(f\"Mean pixel intensity: {mean_intensity:.4f}\")\nprint(f\"Pixel intensity standard deviation: {std_intensity:.4f}\")\n\n\n\n\nMNIST Image Pixel Intensity Distribution\n\n\n\n\nMean pixel intensity: 0.1312\nPixel intensity standard deviation: 0.3089\n\n\n\n\n\nUsing PCA and t-SNE to visualize MNIST data distribution in lower-dimensional space.\n\n# Dimensionality reduction visualization function\ndef visualize_with_dimensionality_reduction(dataset, n_samples=2000):\n    # Randomly select samples\n    indices = np.random.choice(len(dataset), min(n_samples, len(dataset)), replace=False)\n    \n    # Collect data and labels\n    data = []\n    labels = []\n    for idx in indices:\n        img, label = dataset[idx]\n        data.append(img.squeeze().numpy().flatten())  # Flatten 28x28 to 784-dimensional vector\n        labels.append(label)\n    \n    # Convert to numpy arrays\n    X = np.array(data)\n    y = np.array(labels)\n    \n    # Use PCA to reduce to 2 dimensions\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n    \n    # Use t-SNE to reduce to 2 dimensions\n    tsne = TSNE(n_components=2, random_state=42)\n    X_tsne = tsne.fit_transform(X)\n    \n    # Create figure\n    plt.figure(figsize=(16, 7))\n    \n    # PCA scatter plot\n    plt.subplot(1, 2, 1)\n    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', s=5, alpha=0.7)\n    plt.colorbar(scatter, label='Digit Label')\n    plt.title('PCA Dimensionality Reduction (2D)')\n    plt.xlabel(f'Principal Component 1 (Explained Variance: {pca.explained_variance_ratio_[0]:.4f})')\n    plt.ylabel(f'Principal Component 2 (Explained Variance: {pca.explained_variance_ratio_[1]:.4f})')\n    \n    # t-SNE scatter plot\n    plt.subplot(1, 2, 2)\n    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=5, alpha=0.7)\n    plt.colorbar(scatter, label='Digit Label')\n    plt.title('t-SNE Dimensionality Reduction (2D)')\n    plt.xlabel('t-SNE Dimension 1')\n    plt.ylabel('t-SNE Dimension 2')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Return PCA variance explanation ratio\n    return pca.explained_variance_ratio_\n\n# Perform dimensionality reduction visualization on training set\nvar_ratio = visualize_with_dimensionality_reduction(train_dataset)\nprint(f\"Cumulative explained variance ratio of first 10 principal components in PCA: {np.sum(var_ratio):.4f}\")\n\n\n\n\nMNIST Data Visualization through PCA and t-SNE Dimensionality Reduction\n\n\n\n\nCumulative explained variance ratio of first 10 principal components in PCA: 0.1728\n\n\n\n\n\nDifferent digits have variations in shape and strokes. Let’s analyze the average shape and variability of each digit.\n\n# Analyze average shape and variance of each digit\ndef analyze_digit_features(dataset):\n    # Create a dictionary to store all images for each digit\n    digit_images = {i: [] for i in range(10)}\n    \n    # Collect images for each digit\n    for idx in range(len(dataset)):\n        img, label = dataset[idx]\n        img = img.squeeze().numpy()\n        digit_images[label].append(img)\n    \n    # Calculate average image and variance for each digit\n    mean_images = {}\n    var_images = {}\n    for digit, images in digit_images.items():\n        images_array = np.stack(images)\n        mean_images[digit] = np.mean(images_array, axis=0)\n        var_images[digit] = np.var(images_array, axis=0)\n    \n    # Create figure to display average image and variance for each digit\n    plt.figure(figsize=(15, 6))\n    \n    # Display average images\n    plt.subplot(1, 2, 1)\n    # Create a 3x4 grid to display all digits\n    grid_img = np.zeros((28*3, 28*4))\n    \n    for i, digit in enumerate(range(10)):\n        row, col = i // 4, i % 4\n        grid_img[row*28:(row+1)*28, col*28:(col+1)*28] = mean_images[digit]\n    \n    plt.imshow(grid_img, cmap='viridis')\n    plt.title('Average Shape of Each Digit')\n    plt.axis('off')\n    \n    # Display variance images\n    plt.subplot(1, 2, 2)\n    # Create a 3x4 grid to display variance for all digits\n    grid_var = np.zeros((28*3, 28*4))\n    \n    for i, digit in enumerate(range(10)):\n        row, col = i // 4, i % 4\n        grid_var[row*28:(row+1)*28, col*28:(col+1)*28] = var_images[digit]\n    \n    plt.imshow(grid_var, cmap='plasma')\n    plt.title('Pixel Variance of Each Digit')\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return mean_images, var_images\n\n# Analyze digit features\nmean_imgs, var_imgs = analyze_digit_features(train_dataset)\n\n\n\n\nAverage Shape and Variance of Each Digit"
  },
  {
    "objectID": "data-exploration.html#feature-correlation-analysis",
    "href": "data-exploration.html#feature-correlation-analysis",
    "title": "MNIST Dataset Exploration",
    "section": "",
    "text": "Analyze similarities and differences between different digits.\n\n# Calculate similarity between digits\ndef compute_digit_similarities(mean_images):\n    # Calculate 10x10 similarity matrix\n    similarity_matrix = np.zeros((10, 10))\n    \n    # Flatten average images\n    flattened_means = {digit: img.flatten() for digit, img in mean_images.items()}\n    \n    # Calculate correlation coefficient between each pair of digits\n    for i in range(10):\n        for j in range(10):\n            similarity_matrix[i, j] = np.corrcoef(flattened_means[i], flattened_means[j])[0, 1]\n    \n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    plt.imshow(similarity_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n    plt.colorbar(label='Correlation Coefficient')\n    plt.title('Similarity Matrix Between Digits')\n    plt.xlabel('Digit')\n    plt.ylabel('Digit')\n    plt.xticks(range(10))\n    plt.yticks(range(10))\n    \n    # Add correlation coefficient text\n    for i in range(10):\n        for j in range(10):\n            plt.text(j, i, f'{similarity_matrix[i, j]:.2f}', \n                    ha='center', va='center', \n                    color='white' if abs(similarity_matrix[i, j]) &gt; 0.5 else 'black')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return similarity_matrix\n\n# Calculate digit similarity\nsim_matrix = compute_digit_similarities(mean_imgs)\n\n\n\n\nSimilarity Matrix Between Digits"
  },
  {
    "objectID": "data-exploration.html#conclusion",
    "href": "data-exploration.html#conclusion",
    "title": "MNIST Dataset Exploration",
    "section": "",
    "text": "From the above analysis, we can draw the following conclusions about the MNIST dataset:\n\nThe MNIST dataset is well-balanced, with similar distributions of digits in both training and test sets\nThe pixel intensity distribution shows the characteristics of handwritten digits, with most pixels being background (low intensity)\nDimensionality reduction analysis indicates that different digits form distinct clusters in feature space\nSome digits (such as 1 and 7) have higher similarity, while others (such as 0 and 1) have greater differences\nThe average images clearly show the typical shape of each digit"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MNIST Data Exploration",
    "section": "",
    "text": "This website is a tool for MNIST data exploration and model analysis created with Quarto. Here, we will:\n\nExplore the characteristics and distribution of the MNIST dataset\nAnalyze trained model performance\nVisualize prediction results\n\n\n\nMNIST is a handwritten digit recognition dataset containing:\n\n60,000 training images\n10,000 test images\nEach image is a 28x28 pixel grayscale image\n10 classes (digits 0-9)\n\n\n\n\n\nData Exploration: View statistical information and visual analysis of the MNIST dataset\nModel Analysis: Analyze trained model performance and results\n\n\n\nQuarto and Python integration example"
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "MNIST Data Exploration",
    "section": "",
    "text": "MNIST is a handwritten digit recognition dataset containing:\n\n60,000 training images\n10,000 test images\nEach image is a 28x28 pixel grayscale image\n10 classes (digits 0-9)"
  },
  {
    "objectID": "index.html#navigation",
    "href": "index.html#navigation",
    "title": "MNIST Data Exploration",
    "section": "",
    "text": "Data Exploration: View statistical information and visual analysis of the MNIST dataset\nModel Analysis: Analyze trained model performance and results\n\n\n\nQuarto and Python integration example"
  }
]