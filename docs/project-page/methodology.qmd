---
title: "Methodology"
format:
  html:
    code-fold: true
    toc: true
---

# Neural Network Methodology

## Dataset Overview

The MNIST dataset consists of 70,000 grayscale images of handwritten digits (0-9), split into:

- 60,000 training images
- 10,000 test images
- Each image is 28×28 pixels

```{python}
#| echo: true
#| code-fold: false

import matplotlib.pyplot as plt
import numpy as np
import torch
from torchvision import datasets, transforms

# Define the transformation (same as in the project)
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# Load the MNIST dataset using PyTorch
train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)

# Print basic dataset information
print(f"Training data size: {len(train_dataset)} samples")
print(f"Test data size: {len(test_dataset)} samples")
print(f"Image shape: {train_dataset[0][0].shape}")
print(f"Number of classes: {len(train_dataset.classes)}")

# Display sample images
plt.figure(figsize=(10, 2))
for i in range(5):
    # Get a sample
    image, label = train_dataset[i]
    # Convert tensor to numpy array for plotting
    image = image.squeeze().numpy()
    
    # Plot
    plt.subplot(1, 5, i+1)
    plt.imshow(image, cmap='gray')
    plt.title(f"Label: {label}")
    plt.axis('off')

plt.tight_layout()
plt.show()
```

## Model Architecture

Our model is a convolutional neural network with the following architecture:

- Input layer (28×28×1)
- 2D Convolutional layer (32 filters, 3×3 kernel, ReLU activation)
- MaxPooling layer (2×2)
- 2D Convolutional layer (64 filters, 3×3 kernel, ReLU activation)
- MaxPooling layer (2×2)
- Flatten layer
- Dense layer (128 neurons, ReLU activation)
- Dropout layer (0.5)
- Output layer (10 neurons, softmax activation)

```{python}
#| eval: false
import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    """CNN model for MNIST classification"""
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)  # First convolutional layer: 1 channel in, 32 out, 3x3 kernel
        self.conv2 = nn.Conv2d(32, 64, 3, 1)  # Second convolutional layer: 32 channels in, 64 out, 3x3 kernel
        self.dropout1 = nn.Dropout(0.25)  # Dropout layer with 0.25 probability
        self.dropout2 = nn.Dropout(0.5)   # Dropout layer with 0.5 probability
        self.fc1 = nn.Linear(9216, 128)   # First fully connected layer
        self.fc2 = nn.Linear(128, 10)     # Output layer: 10 classes for digits 0-9

    def forward(self, x):
        """Forward pass through the network"""
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)  # Flatten all dimensions except batch
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)  # Apply log softmax for NLL loss
        return output
```

## Training Process

The model was trained with the following parameters:

- **Optimizer**: Adadelta (learning rate = 1.0)
- **Loss function**: Negative log likelihood
- **Batch size**: 64
- **Epochs**: 5
- **Learning rate scheduler**: StepLR with gamma = 0.7

## Evaluation Metrics

We evaluate our model using:

- **Accuracy**: Percentage of correctly classified images
- **Loss**: Negative log likelihood loss on test dataset
- **Confusion Matrix**: Visualization of model predictions vs. actual labels
- **Precision and Recall**: For each digit class

## Data Preprocessing

Before training, we:

1. Normalize the pixel values with mean=0.1307 and std=0.3081
2. Convert images to PyTorch tensors
3. Automatically handle batching with DataLoader

## Hyperparameter Tuning

We conducted grid search over:

- Learning rates: [0.5, 1.0, 1.5]
- Dropout rates: [0.25, 0.5, 0.7]
- Batch sizes: [32, 64, 128] 