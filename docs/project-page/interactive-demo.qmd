---
title: "Interactive Demo"
format:
  html:
    code-fold: true
    toc: true
    css: styles.css
---

# Interactive MNIST Neural Network Explorer

This interactive demo allows you to explore the MNIST neural network in real-time. You can:

1. Draw your own digits and see the model's predictions
2. Explore the training process visually
3. Analyze model internals, like weight distributions and activations

## Draw Your Own Digit

Use the canvas below to draw a digit. The model will predict what digit you've drawn in real-time.

```{=html}
<div class="viz-container">
  <div class="row">
    <div class="col-md-6">
      <h3>Drawing Canvas</h3>
      <canvas id="drawing-canvas" width="280" height="280" style="border:1px solid #000000;"></canvas>
      <div class="mt-3">
        <button id="clear-button" class="btn btn-secondary">Clear Canvas</button>
      </div>
    </div>
    <div class="col-md-6">
      <h3>Prediction</h3>
      <div id="prediction-result" style="font-size: 24px; font-weight: bold;">Draw a digit...</div>
      <h4>Confidence Scores</h4>
      <div id="confidence-bars"></div>
    </div>
  </div>
</div>
```

## Training Visualization

Watch how the neural network learns over time. You can see the accuracy and loss curves update in real-time as the model trains on the MNIST dataset.

```{python}
#| echo: false
#| fig-cap: "Training Visualization"

# This is placeholder Python code for the visualization
# The actual implementation will use JavaScript and TensorFlow.js
import matplotlib.pyplot as plt
import numpy as np

# Generate placeholder data
epochs = np.arange(1, 21)
train_acc = 1 - 0.9 * np.exp(-0.2 * epochs)
val_acc = 1 - 0.9 * np.exp(-0.18 * epochs) - 0.03 * np.random.rand(len(epochs))
train_loss = 2.3 * np.exp(-0.25 * epochs)
val_loss = 2.3 * np.exp(-0.22 * epochs) + 0.05 * np.random.rand(len(epochs))

# Create a figure with two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Plot accuracy
ax1.plot(epochs, train_acc, 'b-', label='Training Accuracy')
ax1.plot(epochs, val_acc, 'r-', label='Validation Accuracy')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Accuracy')
ax1.set_title('Accuracy over Epochs')
ax1.legend()
ax1.grid(True, linestyle='--', alpha=0.7)

# Plot loss
ax2.plot(epochs, train_loss, 'b-', label='Training Loss')
ax2.plot(epochs, val_loss, 'r-', label='Validation Loss')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Loss')
ax2.set_title('Loss over Epochs')
ax2.legend()
ax2.grid(True, linestyle='--', alpha=0.7)

plt.tight_layout()
```

## Weight Visualization

Explore the weights of the neural network as they change during training.

```{=html}
<div class="viz-container">
  <h3>Convolutional Layer Filters</h3>
  <div id="conv-filters" style="min-height: 200px;"></div>
  
  <h3>Weight Distribution</h3>
  <div id="weight-distribution" style="min-height: 300px;"></div>
  
  <div class="control-panel mt-4">
    <label for="layer-select">Select Layer:</label>
    <select id="layer-select" class="form-select">
      <option value="conv1">Convolutional Layer 1</option>
      <option value="conv2">Convolutional Layer 2</option>
      <option value="dense1">Dense Layer 1</option>
      <option value="dense2">Dense Layer 2 (Output)</option>
    </select>
    
    <label for="epoch-slider" class="mt-3">Training Epoch:</label>
    <input type="range" class="form-range" min="0" max="20" value="20" id="epoch-slider">
    <span id="epoch-display">20</span>
  </div>
</div>
```

## Activation Visualization

See how the neural network "sees" different digits by visualizing the activations at each layer.

```{=html}
<div class="viz-container">
  <div class="row mb-4">
    <div class="col-md-12">
      <h3>Sample Digits</h3>
      <div id="sample-digits" class="d-flex justify-content-between"></div>
    </div>
  </div>
  
  <div class="row">
    <div class="col-md-12">
      <h3>Layer Activations</h3>
      <div id="layer-activations"></div>
    </div>
  </div>
</div>
```

## JavaScript for Interactive Elements

```{=html}
<script>
// This JavaScript code would implement the interactive elements
// Including canvas drawing, real-time prediction, and visualizations
// We're providing a skeleton here, but the actual implementation would use TensorFlow.js

document.addEventListener('DOMContentLoaded', function() {
  // Drawing canvas setup
  const canvas = document.getElementById('drawing-canvas');
  const ctx = canvas.getContext('2d');
  let isDrawing = false;
  
  // Set up drawing environment
  ctx.lineWidth = 15;
  ctx.lineCap = 'round';
  ctx.lineJoin = 'round';
  ctx.strokeStyle = 'black';
  
  // Clear canvas button
  document.getElementById('clear-button').addEventListener('click', function() {
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    document.getElementById('prediction-result').textContent = 'Draw a digit...';
    document.getElementById('confidence-bars').innerHTML = '';
  });
  
  // Drawing event listeners
  canvas.addEventListener('mousedown', startDrawing);
  canvas.addEventListener('mousemove', draw);
  canvas.addEventListener('mouseup', stopDrawing);
  canvas.addEventListener('mouseout', stopDrawing);
  
  function startDrawing(e) {
    isDrawing = true;
    draw(e);
  }
  
  function draw(e) {
    if (!isDrawing) return;
    
    const rect = canvas.getBoundingClientRect();
    const x = e.clientX - rect.left;
    const y = e.clientY - rect.top;
    
    ctx.beginPath();
    ctx.moveTo(lastX || x, lastY || y);
    ctx.lineTo(x, y);
    ctx.stroke();
    
    [lastX, lastY] = [x, y];
    
    // In a real implementation, we would call the prediction function here
    // For demonstration, we'll use a placeholder
    debounce(makePrediction, 100)();
  }
  
  function stopDrawing() {
    isDrawing = false;
    [lastX, lastY] = [undefined, undefined];
    makePrediction();
  }
  
  // Placeholder for the prediction function
  function makePrediction() {
    // In a real implementation, this would:
    // 1. Process the canvas image
    // 2. Run inference through the TensorFlow.js model
    // 3. Update the prediction display
    
    // For demonstration, we'll just show random predictions
    const randomPrediction = Math.floor(Math.random() * 10);
    document.getElementById('prediction-result').textContent = `Predicted: ${randomPrediction}`;
    
    // Generate random confidence scores
    let confidences = Array.from({length: 10}, () => Math.random());
    const sum = confidences.reduce((a, b) => a + b, 0);
    confidences = confidences.map(c => c / sum);
    
    // Display confidence bars
    let barsHTML = '';
    for (let i = 0; i < 10; i++) {
      const width = Math.round(confidences[i] * 100);
      const barClass = i === randomPrediction ? 'bg-success' : 'bg-secondary';
      barsHTML += `
        <div class="d-flex align-items-center mb-2">
          <div style="width: 20px;">${i}</div>
          <div class="flex-grow-1">
            <div class="progress">
              <div class="progress-bar ${barClass}" role="progressbar" style="width: ${width}%;" 
                   aria-valuenow="${width}" aria-valuemin="0" aria-valuemax="100">${width}%</div>
            </div>
          </div>
        </div>
      `;
    }
    document.getElementById('confidence-bars').innerHTML = barsHTML;
  }
  
  // Debounce function to prevent too many predictions while drawing
  function debounce(func, wait) {
    let timeout;
    return function() {
      clearTimeout(timeout);
      timeout = setTimeout(() => func.apply(this, arguments), wait);
    };
  }
  
  // Additional code for the other visualizations would go here
  // In a real implementation, this would include code to:
  // - Load and run a TensorFlow.js model
  // - Create interactive visualizations for weights and activations
  // - Handle the epoch slider and layer selection
});
</script>
```

## How It Works

This demo uses [TensorFlow.js](https://www.tensorflow.org/js) to run neural network inference directly in your browser. The model is a pre-trained convolutional neural network with the following architecture:

- Input layer (28×28×1)
- Convolutional layer (32 filters, 3×3 kernel, ReLU activation)
- Max pooling (2×2)
- Convolutional layer (64 filters, 3×3 kernel, ReLU activation)
- Max pooling (2×2)
- Flatten layer
- Dense layer (128 neurons, ReLU activation)
- Dropout (0.5)
- Output layer (10 neurons, softmax activation)

The model achieves approximately 99% accuracy on the MNIST test set. 