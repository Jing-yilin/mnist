{"title":"MNIST Dataset Exploration","markdown":{"yaml":{"title":"MNIST Dataset Exploration","author":"Data Science Team","format":{"html":{"code-fold":false,"toc":true}}},"headingText":"MNIST Dataset Exploration","containsRefs":false,"markdown":"\n\n\nThis document explores the characteristics and statistics of the MNIST dataset. We will load the data and perform visual analysis.\n\n## Loading Data\n\nFirst, we need to import the necessary libraries and load the MNIST dataset.\n\n```{python}\n#| label: load-libraries\nimport sys\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torchvision import datasets, transforms\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# Set matplotlib style\nplt.style.use('ggplot')\n```\n\n```{python}\n#| label: load-data\n\n# Define data transformation\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# Load MNIST dataset\ndata_dir = '../data'\ntrain_dataset = datasets.MNIST(data_dir, train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\n\nprint(f\"Training set size: {len(train_dataset)} samples\")\nprint(f\"Test set size: {len(test_dataset)} samples\")\n```\n\n## Data Visualization\n\n### Viewing Sample Images\n\nLet's visualize some MNIST image samples to understand the characteristics of the data.\n\n```{python}\n#| label: visualize-samples\n#| fig-cap: \"MNIST Dataset Sample Images\"\n\n# Create a function to display images\ndef show_images(dataset, num_images=25, rows=5, cols=5):\n    # Create a new figure\n    plt.figure(figsize=(12, 12))\n    \n    # Randomly select images\n    indices = np.random.choice(len(dataset), num_images, replace=False)\n    \n    # Display images\n    for i, idx in enumerate(indices):\n        if i >= num_images:\n            break\n        \n        # Get image and label\n        img, label = dataset[idx]\n        img = img.squeeze().numpy()  # Convert to numpy and remove channel dimension\n        \n        # Create subplot\n        plt.subplot(rows, cols, i + 1)\n        plt.imshow(img, cmap='gray')\n        plt.title(f'Label: {label}')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Display some images from the training set\nshow_images(train_dataset)\n```\n\n### Label Distribution\n\nLet's look at the distribution of digits in the training and test sets.\n\n```{python}\n#| label: label-distribution\n#| fig-cap: \"Label Distribution in Training and Test Sets\"\n\n# Get all labels\ntrain_labels = [label for _, label in train_dataset]\ntest_labels = [label for _, label in test_dataset]\n\n# Calculate frequency of each digit\ndef plot_label_distribution(train_labels, test_labels):\n    train_counts = np.bincount(train_labels)\n    test_counts = np.bincount(test_labels)\n    \n    # Create dataframe\n    df = pd.DataFrame({\n        'Training Set': train_counts,\n        'Test Set': test_counts\n    }, index=range(10))\n    \n    # Create stacked bar chart\n    ax = df.plot(kind='bar', figsize=(12, 6), rot=0)\n    plt.title('Number of Samples for Each Digit in MNIST Dataset')\n    plt.xlabel('Digit')\n    plt.ylabel('Number of Samples')\n    plt.xticks(range(10), [str(i) for i in range(10)])\n    \n    # Add value labels to each bar\n    for container in ax.containers:\n        ax.bar_label(container, fmt='%d')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Return percentage distribution\n    train_pct = (train_counts / len(train_labels) * 100).round(2)\n    test_pct = (test_counts / len(test_labels) * 100).round(2)\n    \n    pct_df = pd.DataFrame({\n        'Training Set (%)': train_pct,\n        'Test Set (%)': test_pct\n    }, index=range(10))\n    \n    return pct_df\n\n# Plot label distribution and display percentage table\npct_table = plot_label_distribution(train_labels, test_labels)\npct_table\n```\n\n### Pixel Intensity Distribution\n\nLet's analyze the pixel intensity distribution of MNIST images.\n\n```{python}\n#| label: pixel-intensity\n#| fig-cap: \"MNIST Image Pixel Intensity Distribution\"\n\n# Create a function to calculate pixel intensity statistics\ndef analyze_pixel_intensity(dataset, num_samples=1000):\n    # Randomly select samples\n    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n    \n    # Collect images\n    images = []\n    for idx in indices:\n        img, _ = dataset[idx]\n        # Remove normalization to get original pixel values\n        img = img * 0.3081 + 0.1307  # De-normalize\n        images.append(img.squeeze().numpy())\n    \n    # Stack images into a large array\n    images_array = np.stack(images)\n    \n    # Calculate average pixel intensity for each image\n    mean_intensities = images_array.mean(axis=(1, 2))\n    \n    # Calculate overall mean intensity\n    overall_mean = images_array.mean()\n    overall_std = images_array.std()\n    \n    # Create histograms\n    plt.figure(figsize=(12, 6))\n    \n    # Mean intensity histogram\n    plt.subplot(1, 2, 1)\n    plt.hist(mean_intensities, bins=30, alpha=0.7, color='blue')\n    plt.axvline(overall_mean, color='red', linestyle='dashed', linewidth=2)\n    plt.title(f'Mean Pixel Intensity Distribution\\nMean: {overall_mean:.4f}')\n    plt.xlabel('Mean Pixel Intensity')\n    plt.ylabel('Number of Images')\n    \n    # All pixel intensities histogram\n    plt.subplot(1, 2, 2)\n    plt.hist(images_array.flatten(), bins=50, alpha=0.7, color='green')\n    plt.title(f'All Pixel Intensity Distribution\\nStandard Deviation: {overall_std:.4f}')\n    plt.xlabel('Pixel Intensity')\n    plt.ylabel('Frequency')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return overall_mean, overall_std\n\n# Analyze pixel intensity of training set\nmean_intensity, std_intensity = analyze_pixel_intensity(train_dataset)\nprint(f\"Mean pixel intensity: {mean_intensity:.4f}\")\nprint(f\"Pixel intensity standard deviation: {std_intensity:.4f}\")\n```\n\n### Dimensionality Reduction Visualization\n\nUsing PCA and t-SNE to visualize MNIST data distribution in lower-dimensional space.\n\n```{python}\n#| label: dimensionality-reduction\n#| fig-cap: \"MNIST Data Visualization through PCA and t-SNE Dimensionality Reduction\"\n\n# Dimensionality reduction visualization function\ndef visualize_with_dimensionality_reduction(dataset, n_samples=2000):\n    # Randomly select samples\n    indices = np.random.choice(len(dataset), min(n_samples, len(dataset)), replace=False)\n    \n    # Collect data and labels\n    data = []\n    labels = []\n    for idx in indices:\n        img, label = dataset[idx]\n        data.append(img.squeeze().numpy().flatten())  # Flatten 28x28 to 784-dimensional vector\n        labels.append(label)\n    \n    # Convert to numpy arrays\n    X = np.array(data)\n    y = np.array(labels)\n    \n    # Use PCA to reduce to 2 dimensions\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n    \n    # Use t-SNE to reduce to 2 dimensions\n    tsne = TSNE(n_components=2, random_state=42)\n    X_tsne = tsne.fit_transform(X)\n    \n    # Create figure\n    plt.figure(figsize=(16, 7))\n    \n    # PCA scatter plot\n    plt.subplot(1, 2, 1)\n    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', s=5, alpha=0.7)\n    plt.colorbar(scatter, label='Digit Label')\n    plt.title('PCA Dimensionality Reduction (2D)')\n    plt.xlabel(f'Principal Component 1 (Explained Variance: {pca.explained_variance_ratio_[0]:.4f})')\n    plt.ylabel(f'Principal Component 2 (Explained Variance: {pca.explained_variance_ratio_[1]:.4f})')\n    \n    # t-SNE scatter plot\n    plt.subplot(1, 2, 2)\n    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=5, alpha=0.7)\n    plt.colorbar(scatter, label='Digit Label')\n    plt.title('t-SNE Dimensionality Reduction (2D)')\n    plt.xlabel('t-SNE Dimension 1')\n    plt.ylabel('t-SNE Dimension 2')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Return PCA variance explanation ratio\n    return pca.explained_variance_ratio_\n\n# Perform dimensionality reduction visualization on training set\nvar_ratio = visualize_with_dimensionality_reduction(train_dataset)\nprint(f\"Cumulative explained variance ratio of first 10 principal components in PCA: {np.sum(var_ratio):.4f}\")\n```\n\n### Digit Feature Analysis\n\nDifferent digits have variations in shape and strokes. Let's analyze the average shape and variability of each digit.\n\n```{python}\n#| label: digit-features\n#| fig-cap: \"Average Shape and Variance of Each Digit\"\n\n# Analyze average shape and variance of each digit\ndef analyze_digit_features(dataset):\n    # Create a dictionary to store all images for each digit\n    digit_images = {i: [] for i in range(10)}\n    \n    # Collect images for each digit\n    for idx in range(len(dataset)):\n        img, label = dataset[idx]\n        img = img.squeeze().numpy()\n        digit_images[label].append(img)\n    \n    # Calculate average image and variance for each digit\n    mean_images = {}\n    var_images = {}\n    for digit, images in digit_images.items():\n        images_array = np.stack(images)\n        mean_images[digit] = np.mean(images_array, axis=0)\n        var_images[digit] = np.var(images_array, axis=0)\n    \n    # Create figure to display average image and variance for each digit\n    plt.figure(figsize=(15, 6))\n    \n    # Display average images\n    plt.subplot(1, 2, 1)\n    # Create a 3x4 grid to display all digits\n    grid_img = np.zeros((28*3, 28*4))\n    \n    for i, digit in enumerate(range(10)):\n        row, col = i // 4, i % 4\n        grid_img[row*28:(row+1)*28, col*28:(col+1)*28] = mean_images[digit]\n    \n    plt.imshow(grid_img, cmap='viridis')\n    plt.title('Average Shape of Each Digit')\n    plt.axis('off')\n    \n    # Display variance images\n    plt.subplot(1, 2, 2)\n    # Create a 3x4 grid to display variance for all digits\n    grid_var = np.zeros((28*3, 28*4))\n    \n    for i, digit in enumerate(range(10)):\n        row, col = i // 4, i % 4\n        grid_var[row*28:(row+1)*28, col*28:(col+1)*28] = var_images[digit]\n    \n    plt.imshow(grid_var, cmap='plasma')\n    plt.title('Pixel Variance of Each Digit')\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return mean_images, var_images\n\n# Analyze digit features\nmean_imgs, var_imgs = analyze_digit_features(train_dataset)\n```\n\n## Feature Correlation Analysis\n\nAnalyze similarities and differences between different digits.\n\n```{python}\n#| label: feature-correlation\n#| fig-cap: \"Similarity Matrix Between Digits\"\n\n# Calculate similarity between digits\ndef compute_digit_similarities(mean_images):\n    # Calculate 10x10 similarity matrix\n    similarity_matrix = np.zeros((10, 10))\n    \n    # Flatten average images\n    flattened_means = {digit: img.flatten() for digit, img in mean_images.items()}\n    \n    # Calculate correlation coefficient between each pair of digits\n    for i in range(10):\n        for j in range(10):\n            similarity_matrix[i, j] = np.corrcoef(flattened_means[i], flattened_means[j])[0, 1]\n    \n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    plt.imshow(similarity_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n    plt.colorbar(label='Correlation Coefficient')\n    plt.title('Similarity Matrix Between Digits')\n    plt.xlabel('Digit')\n    plt.ylabel('Digit')\n    plt.xticks(range(10))\n    plt.yticks(range(10))\n    \n    # Add correlation coefficient text\n    for i in range(10):\n        for j in range(10):\n            plt.text(j, i, f'{similarity_matrix[i, j]:.2f}', \n                    ha='center', va='center', \n                    color='white' if abs(similarity_matrix[i, j]) > 0.5 else 'black')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return similarity_matrix\n\n# Calculate digit similarity\nsim_matrix = compute_digit_similarities(mean_imgs)\n```\n\n## Conclusion\n\nFrom the above analysis, we can draw the following conclusions about the MNIST dataset:\n\n1. The MNIST dataset is well-balanced, with similar distributions of digits in both training and test sets\n2. The pixel intensity distribution shows the characteristics of handwritten digits, with most pixels being background (low intensity)\n3. Dimensionality reduction analysis indicates that different digits form distinct clusters in feature space\n4. Some digits (such as 1 and 7) have higher similarity, while others (such as 0 and 1) have greater differences\n5. The average images clearly show the typical shape of each digit ","srcMarkdownNoYaml":"\n\n# MNIST Dataset Exploration\n\nThis document explores the characteristics and statistics of the MNIST dataset. We will load the data and perform visual analysis.\n\n## Loading Data\n\nFirst, we need to import the necessary libraries and load the MNIST dataset.\n\n```{python}\n#| label: load-libraries\nimport sys\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torchvision import datasets, transforms\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# Set matplotlib style\nplt.style.use('ggplot')\n```\n\n```{python}\n#| label: load-data\n\n# Define data transformation\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# Load MNIST dataset\ndata_dir = '../data'\ntrain_dataset = datasets.MNIST(data_dir, train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\n\nprint(f\"Training set size: {len(train_dataset)} samples\")\nprint(f\"Test set size: {len(test_dataset)} samples\")\n```\n\n## Data Visualization\n\n### Viewing Sample Images\n\nLet's visualize some MNIST image samples to understand the characteristics of the data.\n\n```{python}\n#| label: visualize-samples\n#| fig-cap: \"MNIST Dataset Sample Images\"\n\n# Create a function to display images\ndef show_images(dataset, num_images=25, rows=5, cols=5):\n    # Create a new figure\n    plt.figure(figsize=(12, 12))\n    \n    # Randomly select images\n    indices = np.random.choice(len(dataset), num_images, replace=False)\n    \n    # Display images\n    for i, idx in enumerate(indices):\n        if i >= num_images:\n            break\n        \n        # Get image and label\n        img, label = dataset[idx]\n        img = img.squeeze().numpy()  # Convert to numpy and remove channel dimension\n        \n        # Create subplot\n        plt.subplot(rows, cols, i + 1)\n        plt.imshow(img, cmap='gray')\n        plt.title(f'Label: {label}')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Display some images from the training set\nshow_images(train_dataset)\n```\n\n### Label Distribution\n\nLet's look at the distribution of digits in the training and test sets.\n\n```{python}\n#| label: label-distribution\n#| fig-cap: \"Label Distribution in Training and Test Sets\"\n\n# Get all labels\ntrain_labels = [label for _, label in train_dataset]\ntest_labels = [label for _, label in test_dataset]\n\n# Calculate frequency of each digit\ndef plot_label_distribution(train_labels, test_labels):\n    train_counts = np.bincount(train_labels)\n    test_counts = np.bincount(test_labels)\n    \n    # Create dataframe\n    df = pd.DataFrame({\n        'Training Set': train_counts,\n        'Test Set': test_counts\n    }, index=range(10))\n    \n    # Create stacked bar chart\n    ax = df.plot(kind='bar', figsize=(12, 6), rot=0)\n    plt.title('Number of Samples for Each Digit in MNIST Dataset')\n    plt.xlabel('Digit')\n    plt.ylabel('Number of Samples')\n    plt.xticks(range(10), [str(i) for i in range(10)])\n    \n    # Add value labels to each bar\n    for container in ax.containers:\n        ax.bar_label(container, fmt='%d')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Return percentage distribution\n    train_pct = (train_counts / len(train_labels) * 100).round(2)\n    test_pct = (test_counts / len(test_labels) * 100).round(2)\n    \n    pct_df = pd.DataFrame({\n        'Training Set (%)': train_pct,\n        'Test Set (%)': test_pct\n    }, index=range(10))\n    \n    return pct_df\n\n# Plot label distribution and display percentage table\npct_table = plot_label_distribution(train_labels, test_labels)\npct_table\n```\n\n### Pixel Intensity Distribution\n\nLet's analyze the pixel intensity distribution of MNIST images.\n\n```{python}\n#| label: pixel-intensity\n#| fig-cap: \"MNIST Image Pixel Intensity Distribution\"\n\n# Create a function to calculate pixel intensity statistics\ndef analyze_pixel_intensity(dataset, num_samples=1000):\n    # Randomly select samples\n    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n    \n    # Collect images\n    images = []\n    for idx in indices:\n        img, _ = dataset[idx]\n        # Remove normalization to get original pixel values\n        img = img * 0.3081 + 0.1307  # De-normalize\n        images.append(img.squeeze().numpy())\n    \n    # Stack images into a large array\n    images_array = np.stack(images)\n    \n    # Calculate average pixel intensity for each image\n    mean_intensities = images_array.mean(axis=(1, 2))\n    \n    # Calculate overall mean intensity\n    overall_mean = images_array.mean()\n    overall_std = images_array.std()\n    \n    # Create histograms\n    plt.figure(figsize=(12, 6))\n    \n    # Mean intensity histogram\n    plt.subplot(1, 2, 1)\n    plt.hist(mean_intensities, bins=30, alpha=0.7, color='blue')\n    plt.axvline(overall_mean, color='red', linestyle='dashed', linewidth=2)\n    plt.title(f'Mean Pixel Intensity Distribution\\nMean: {overall_mean:.4f}')\n    plt.xlabel('Mean Pixel Intensity')\n    plt.ylabel('Number of Images')\n    \n    # All pixel intensities histogram\n    plt.subplot(1, 2, 2)\n    plt.hist(images_array.flatten(), bins=50, alpha=0.7, color='green')\n    plt.title(f'All Pixel Intensity Distribution\\nStandard Deviation: {overall_std:.4f}')\n    plt.xlabel('Pixel Intensity')\n    plt.ylabel('Frequency')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return overall_mean, overall_std\n\n# Analyze pixel intensity of training set\nmean_intensity, std_intensity = analyze_pixel_intensity(train_dataset)\nprint(f\"Mean pixel intensity: {mean_intensity:.4f}\")\nprint(f\"Pixel intensity standard deviation: {std_intensity:.4f}\")\n```\n\n### Dimensionality Reduction Visualization\n\nUsing PCA and t-SNE to visualize MNIST data distribution in lower-dimensional space.\n\n```{python}\n#| label: dimensionality-reduction\n#| fig-cap: \"MNIST Data Visualization through PCA and t-SNE Dimensionality Reduction\"\n\n# Dimensionality reduction visualization function\ndef visualize_with_dimensionality_reduction(dataset, n_samples=2000):\n    # Randomly select samples\n    indices = np.random.choice(len(dataset), min(n_samples, len(dataset)), replace=False)\n    \n    # Collect data and labels\n    data = []\n    labels = []\n    for idx in indices:\n        img, label = dataset[idx]\n        data.append(img.squeeze().numpy().flatten())  # Flatten 28x28 to 784-dimensional vector\n        labels.append(label)\n    \n    # Convert to numpy arrays\n    X = np.array(data)\n    y = np.array(labels)\n    \n    # Use PCA to reduce to 2 dimensions\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n    \n    # Use t-SNE to reduce to 2 dimensions\n    tsne = TSNE(n_components=2, random_state=42)\n    X_tsne = tsne.fit_transform(X)\n    \n    # Create figure\n    plt.figure(figsize=(16, 7))\n    \n    # PCA scatter plot\n    plt.subplot(1, 2, 1)\n    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', s=5, alpha=0.7)\n    plt.colorbar(scatter, label='Digit Label')\n    plt.title('PCA Dimensionality Reduction (2D)')\n    plt.xlabel(f'Principal Component 1 (Explained Variance: {pca.explained_variance_ratio_[0]:.4f})')\n    plt.ylabel(f'Principal Component 2 (Explained Variance: {pca.explained_variance_ratio_[1]:.4f})')\n    \n    # t-SNE scatter plot\n    plt.subplot(1, 2, 2)\n    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=5, alpha=0.7)\n    plt.colorbar(scatter, label='Digit Label')\n    plt.title('t-SNE Dimensionality Reduction (2D)')\n    plt.xlabel('t-SNE Dimension 1')\n    plt.ylabel('t-SNE Dimension 2')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Return PCA variance explanation ratio\n    return pca.explained_variance_ratio_\n\n# Perform dimensionality reduction visualization on training set\nvar_ratio = visualize_with_dimensionality_reduction(train_dataset)\nprint(f\"Cumulative explained variance ratio of first 10 principal components in PCA: {np.sum(var_ratio):.4f}\")\n```\n\n### Digit Feature Analysis\n\nDifferent digits have variations in shape and strokes. Let's analyze the average shape and variability of each digit.\n\n```{python}\n#| label: digit-features\n#| fig-cap: \"Average Shape and Variance of Each Digit\"\n\n# Analyze average shape and variance of each digit\ndef analyze_digit_features(dataset):\n    # Create a dictionary to store all images for each digit\n    digit_images = {i: [] for i in range(10)}\n    \n    # Collect images for each digit\n    for idx in range(len(dataset)):\n        img, label = dataset[idx]\n        img = img.squeeze().numpy()\n        digit_images[label].append(img)\n    \n    # Calculate average image and variance for each digit\n    mean_images = {}\n    var_images = {}\n    for digit, images in digit_images.items():\n        images_array = np.stack(images)\n        mean_images[digit] = np.mean(images_array, axis=0)\n        var_images[digit] = np.var(images_array, axis=0)\n    \n    # Create figure to display average image and variance for each digit\n    plt.figure(figsize=(15, 6))\n    \n    # Display average images\n    plt.subplot(1, 2, 1)\n    # Create a 3x4 grid to display all digits\n    grid_img = np.zeros((28*3, 28*4))\n    \n    for i, digit in enumerate(range(10)):\n        row, col = i // 4, i % 4\n        grid_img[row*28:(row+1)*28, col*28:(col+1)*28] = mean_images[digit]\n    \n    plt.imshow(grid_img, cmap='viridis')\n    plt.title('Average Shape of Each Digit')\n    plt.axis('off')\n    \n    # Display variance images\n    plt.subplot(1, 2, 2)\n    # Create a 3x4 grid to display variance for all digits\n    grid_var = np.zeros((28*3, 28*4))\n    \n    for i, digit in enumerate(range(10)):\n        row, col = i // 4, i % 4\n        grid_var[row*28:(row+1)*28, col*28:(col+1)*28] = var_images[digit]\n    \n    plt.imshow(grid_var, cmap='plasma')\n    plt.title('Pixel Variance of Each Digit')\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return mean_images, var_images\n\n# Analyze digit features\nmean_imgs, var_imgs = analyze_digit_features(train_dataset)\n```\n\n## Feature Correlation Analysis\n\nAnalyze similarities and differences between different digits.\n\n```{python}\n#| label: feature-correlation\n#| fig-cap: \"Similarity Matrix Between Digits\"\n\n# Calculate similarity between digits\ndef compute_digit_similarities(mean_images):\n    # Calculate 10x10 similarity matrix\n    similarity_matrix = np.zeros((10, 10))\n    \n    # Flatten average images\n    flattened_means = {digit: img.flatten() for digit, img in mean_images.items()}\n    \n    # Calculate correlation coefficient between each pair of digits\n    for i in range(10):\n        for j in range(10):\n            similarity_matrix[i, j] = np.corrcoef(flattened_means[i], flattened_means[j])[0, 1]\n    \n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    plt.imshow(similarity_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n    plt.colorbar(label='Correlation Coefficient')\n    plt.title('Similarity Matrix Between Digits')\n    plt.xlabel('Digit')\n    plt.ylabel('Digit')\n    plt.xticks(range(10))\n    plt.yticks(range(10))\n    \n    # Add correlation coefficient text\n    for i in range(10):\n        for j in range(10):\n            plt.text(j, i, f'{similarity_matrix[i, j]:.2f}', \n                    ha='center', va='center', \n                    color='white' if abs(similarity_matrix[i, j]) > 0.5 else 'black')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return similarity_matrix\n\n# Calculate digit similarity\nsim_matrix = compute_digit_similarities(mean_imgs)\n```\n\n## Conclusion\n\nFrom the above analysis, we can draw the following conclusions about the MNIST dataset:\n\n1. The MNIST dataset is well-balanced, with similar distributions of digits in both training and test sets\n2. The pixel intensity distribution shows the characteristics of handwritten digits, with most pixels being background (low intensity)\n3. Dimensionality reduction analysis indicates that different digits form distinct clusters in feature space\n4. Some digits (such as 1 and 7) have higher similarity, while others (such as 0 and 1) have greater differences\n5. The average images clearly show the typical shape of each digit "},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"data-exploration.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.43","theme":"cosmo","title":"MNIST Dataset Exploration","author":"Data Science Team"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}