{
  "hash": "bac4747adf8a604fc0b52ddd9a0f39d1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"MNIST Model Analysis\"\nauthor: \"Data Science Team\"\nformat:\n  html:\n    code-fold: false\n    toc: true\n---\n\n\n\n\n# MNIST Model Analysis\n\nIn this document, we will analyze the performance and prediction results of trained MNIST classification models.\n\n## Loading Necessary Libraries\n\n::: {#load-libraries .cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nimport json\n\n# Set matplotlib style\nplt.style.use('ggplot')\n```\n:::\n\n\n## Define Model Structure\n\nFirst, we need to define the same model structure used during training.\n\n::: {#define-model .cell execution_count=2}\n``` {.python .cell-code}\nclass Net(nn.Module):\n    \"\"\"CNN model for MNIST classification\"\"\"\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)  # First convolutional layer: 1 channel in, 32 out, 3x3 kernel\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)  # Second convolutional layer: 32 channels in, 64 out, 3x3 kernel\n        self.dropout1 = nn.Dropout(0.25)  # Dropout layer with 0.25 probability\n        self.dropout2 = nn.Dropout(0.5)   # Dropout layer with 0.5 probability\n        self.fc1 = nn.Linear(9216, 128)   # First fully connected layer\n        self.fc2 = nn.Linear(128, 10)     # Output layer: 10 classes for digits 0-9\n\n    def forward(self, x):\n        \"\"\"Forward pass through the network\"\"\"\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)  # Apply log softmax for NLL loss\n        return output\n```\n:::\n\n\n## Loading Data and Model\n\n::: {#load-data-model .cell execution_count=3}\n``` {.python .cell-code}\n# Define data transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# Load test dataset\ndata_dir = '../data'\ntest_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000)\n\n# Determine available device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \n                     \"mps\" if torch.backends.mps.is_available() else \n                     \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Try to load the model\ntry:\n    # Try multiple possible environments\n    env_types = [\"default\", \"cpu\", \"cuda\", \"mps\"]\n    model_loaded = False\n    \n    for env_type in env_types:\n        model_path = f\"../models/{env_type}/mnist_cnn.pt\"\n        if os.path.exists(model_path):\n            # Create model instance\n            model = Net().to(device)\n            # Load model weights\n            model.load_state_dict(torch.load(model_path, map_location=device))\n            model.eval()  # Set to evaluation mode\n            print(f\"✅ Successfully loaded model from {model_path}\")\n            model_loaded = True\n            break\n    \n    if not model_loaded:\n        raise FileNotFoundError(\"Could not find trained model file\")\n    \nexcept Exception as e:\n    print(f\"❌ Error loading model: {str(e)}\")\n    print(\"Please use 'pixi run train-model' to train the model first\")\n    # Create untrained model for demonstration\n    model = Net().to(device)\n    model.eval()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing device: mps\n✅ Successfully loaded model from ../models/cpu/mnist_cnn.pt\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/9b/nq3qtb3n0cxgw9m4sy1sycrh0000gn/T/ipykernel_36502/2626477224.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n```\n:::\n:::\n\n\n## Loading Training Results\n\n::: {#load-results .cell execution_count=4}\n``` {.python .cell-code}\n# Try to load results file\ntry:\n    # Try multiple possible environments\n    env_types = [\"default\", \"cpu\", \"cuda\", \"mps\"]\n    results_loaded = False\n    \n    for env_type in env_types:\n        results_path = f\"../results/{env_type}/mnist_results.json\"\n        if os.path.exists(results_path):\n            # Load results data\n            with open(results_path, 'r') as f:\n                results = json.load(f)\n            print(f\"✅ Successfully loaded results data from {results_path}\")\n            results_loaded = True\n            break\n    \n    if not results_loaded:\n        raise FileNotFoundError(\"Could not find training results file\")\n    \nexcept Exception as e:\n    print(f\"❌ Error loading results: {str(e)}\")\n    print(\"Please use 'pixi run train-model' and 'pixi run test-model' to generate results\")\n    # Create empty results dictionary for demonstration\n    results = {\"training_history\": [], \"testing_history\": []}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n✅ Successfully loaded results data from ../results/cpu/mnist_results.json\n```\n:::\n:::\n\n\n## Model Performance Analysis\n\n### Training History\n\nAnalyze the changes in loss and accuracy during the training process.\n\n::: {#cell-training-history .cell execution_count=5}\n``` {.python .cell-code}\nif 'training_history' in results and len(results['training_history']) > 0:\n    # Organize training history data\n    train_data = results['training_history']\n    \n    # Group by epoch to calculate average loss\n    epoch_losses = {}\n    for entry in train_data:\n        epoch = entry['epoch']\n        if epoch not in epoch_losses:\n            epoch_losses[epoch] = []\n        epoch_losses[epoch].append(entry['loss'])\n    \n    # Calculate average loss for each epoch\n    epochs = sorted(epoch_losses.keys())\n    avg_losses = [np.mean(epoch_losses[e]) for e in epochs]\n    \n    # Get test history\n    test_history = results.get('testing_history', [])\n    test_epochs = [entry['epoch'] for entry in test_history]\n    test_losses = [entry['loss'] for entry in test_history]\n    test_accuracies = [entry['accuracy'] for entry in test_history]\n    \n    # Create figure\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Training loss curve\n    ax1.plot(epochs, avg_losses, 'o-', color='blue', label='Training Loss')\n    if test_history:\n        ax1.plot(test_epochs, test_losses, 's-', color='red', label='Test Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('Training and Test Loss')\n    ax1.legend()\n    ax1.grid(True)\n    \n    # Test accuracy curve\n    if test_history:\n        ax2.plot(test_epochs, test_accuracies, 's-', color='green')\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('Accuracy (%)')\n        ax2.set_title('Test Accuracy')\n        ax2.grid(True)\n    else:\n        ax2.text(0.5, 0.5, 'No test accuracy data available', ha='center', va='center')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Display final test results\n    if 'final_test_result' in results:\n        final_result = results['final_test_result']\n        print(f\"Final test loss: {final_result['loss']:.4f}\")\n        print(f\"Final test accuracy: {final_result['accuracy']:.2f}%\")\n        print(f\"Correct predictions: {final_result['correct']}/{final_result['total']}\")\nelse:\n    print(\"No training history data available\")\n```\n\n::: {.cell-output .cell-output-display}\n![Model Training History](model-analysis_files/figure-html/training-history-output-1.png){#training-history width=1527 height=566}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFinal test loss: 0.0276\nFinal test accuracy: 99.11%\nCorrect predictions: 9911/10000\n```\n:::\n:::\n\n\n### Confusion Matrix\n\nCalculate and visualize the model's confusion matrix on the test set.\n\n::: {#cell-confusion-matrix .cell execution_count=6}\n``` {.python .cell-code}\ndef evaluate_model(model, data_loader, device):\n    model.eval()\n    all_preds = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for data, target in data_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            pred = output.argmax(dim=1, keepdim=True).squeeze()\n            all_preds.extend(pred.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n    \n    return np.array(all_preds), np.array(all_targets)\n\n# Evaluate model on test set\ntry:\n    predictions, targets = evaluate_model(model, test_loader, device)\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(targets, predictions)\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n                xticklabels=range(10), yticklabels=range(10))\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.title('Confusion Matrix')\n    plt.tight_layout()\n    plt.show()\n    \n    # Calculate and display classification report\n    report = classification_report(targets, predictions, output_dict=True)\n    report_df = pd.DataFrame(report).transpose()\n    \n    # Filter and rename columns\n    if 'accuracy' in report_df.index:\n        accuracy_row = report_df.loc[['accuracy']]\n        report_df = report_df.drop('accuracy')\n        report_df = report_df.drop('macro avg', errors='ignore')\n        report_df = report_df.drop('weighted avg', errors='ignore')\n    \n    # Display performance for each digit\n    print(\"Classification performance by digit:\")\n    print(report_df.round(3))\n    \nexcept Exception as e:\n    print(f\"Error evaluating model: {str(e)}\")\n```\n\n::: {.cell-output .cell-output-display}\n![Confusion Matrix on Test Set](model-analysis_files/figure-html/confusion-matrix-output-1.png){#confusion-matrix width=950 height=758}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nClassification performance by digit:\n   precision  recall  f1-score  support\n0      0.988   0.998     0.993    980.0\n1      0.996   0.998     0.997   1135.0\n2      0.990   0.991     0.991   1032.0\n3      0.995   0.992     0.994   1010.0\n4      0.995   0.990     0.992    982.0\n5      0.988   0.992     0.990    892.0\n6      0.995   0.983     0.989    958.0\n7      0.989   0.990     0.990   1028.0\n8      0.985   0.990     0.987    974.0\n9      0.989   0.985     0.987   1009.0\n```\n:::\n:::\n\n\n## Prediction Visualization\n\nView prediction results on some test samples.\n\n::: {#cell-prediction-visualization .cell execution_count=7}\n``` {.python .cell-code}\ndef visualize_predictions(model, dataset, device, num_samples=25):\n    # Create data loader\n    loader = torch.utils.data.DataLoader(dataset, batch_size=num_samples)\n    \n    # Get a batch of data\n    data, targets = next(iter(loader))\n    data, targets = data.to(device), targets.to(device)\n    \n    # Get predictions\n    model.eval()\n    with torch.no_grad():\n        outputs = model(data)\n        probs = torch.exp(outputs)\n        preds = outputs.argmax(dim=1)\n    \n    # Move data back to CPU\n    images = data.cpu().numpy()\n    targets = targets.cpu().numpy()\n    preds = preds.cpu().numpy()\n    probs = probs.cpu().numpy()\n    \n    # Create figure\n    rows, cols = 5, 5\n    fig, axes = plt.subplots(rows, cols, figsize=(12, 12))\n    \n    for i, ax in enumerate(axes.flat):\n        if i < num_samples:\n            # Display image\n            img = images[i][0]\n            ax.imshow(img, cmap='gray')\n            \n            # Set title\n            true_label = targets[i]\n            pred_label = preds[i]\n            probability = probs[i][pred_label]\n            \n            title = f\"True: {true_label}, Pred: {pred_label}\"\n            color = 'green' if true_label == pred_label else 'red'\n            \n            ax.set_title(title, color=color)\n            ax.text(0.5, -0.15, f\"Probability: {probability:.2f}\", \n                   transform=ax.transAxes, ha='center')\n            ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ntry:\n    # Visualize some predictions\n    visualize_predictions(model, test_dataset, device)\nexcept Exception as e:\n    print(f\"Error visualizing predictions: {str(e)}\")\n```\n\n::: {.cell-output .cell-output-display}\n![Test Sample Prediction Results](model-analysis_files/figure-html/prediction-visualization-output-1.png){#prediction-visualization width=1093 height=1144}\n:::\n:::\n\n\n## Error Analysis\n\nAnalyze which samples the model tends to misclassify.\n\n::: {#cell-error-analysis .cell execution_count=8}\n``` {.python .cell-code}\ndef analyze_errors(model, dataset, device, num_errors=15):\n    # Create data loader\n    loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n    \n    # Collect error predictions\n    errors = []\n    \n    model.eval()\n    with torch.no_grad():\n        for i, (data, target) in enumerate(loader):\n            if len(errors) >= num_errors:\n                break\n                \n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            pred = output.argmax(dim=1)\n            \n            if pred.item() != target.item():\n                probs = torch.exp(output)\n                errors.append({\n                    'image': data.cpu().squeeze().numpy(),\n                    'true': target.item(),\n                    'pred': pred.item(),\n                    'probs': probs.cpu().numpy()[0],\n                    'index': i\n                })\n    \n    if not errors:\n        print(\"No error predictions found\")\n        return\n    \n    # Create figure\n    rows, cols = 3, 5\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 9))\n    \n    for i, ax in enumerate(axes.flat):\n        if i < len(errors):\n            err = errors[i]\n            \n            # Display image\n            ax.imshow(err['image'], cmap='gray')\n            \n            # Set title\n            title = f\"True: {err['true']}, Pred: {err['pred']}\"\n            ax.set_title(title, color='red')\n            \n            # Display top 3 highest probabilities\n            top_k = np.argsort(err['probs'])[-3:][::-1]\n            probs_text = \"\\n\".join([f\"{j}: {err['probs'][j]:.2f}\" for j in top_k])\n            ax.text(0.95, 0.05, probs_text, \n                   transform=ax.transAxes, ha='right', va='bottom',\n                   bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n            \n            ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ntry:\n    # Analyze error predictions\n    analyze_errors(model, test_dataset, device)\nexcept Exception as e:\n    print(f\"Error analyzing errors: {str(e)}\")\n```\n\n::: {.cell-output .cell-output-display}\n![Error Prediction Sample Analysis](model-analysis_files/figure-html/error-analysis-output-1.png){#error-analysis width=1408 height=856}\n:::\n:::\n\n\n## Feature Importance Analysis\n\nUse gradient visualization to analyze which pixels are most important for the model's decision.\n\n::: {#cell-feature-importance .cell execution_count=9}\n``` {.python .cell-code}\ndef compute_gradients(model, image, target, device):\n    # Add gradient tracking to image\n    image.requires_grad_()\n    \n    # Forward pass\n    model.eval()\n    output = model(image)\n    \n    # Calculate gradient for target class\n    model.zero_grad()\n    one_hot = torch.zeros_like(output)\n    one_hot[0, target] = 1\n    output.backward(gradient=one_hot)\n    \n    # Get gradient\n    return image.grad.data.abs().cpu().numpy()[0][0]\n\ntry:\n    # Select some samples from test set\n    samples = [0, 1000, 2000, 3000, 4000]\n    num_samples = len(samples)\n    \n    # Create figure\n    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n    \n    for i, sample_idx in enumerate(samples):\n        # Get sample\n        image, target = test_dataset[sample_idx]\n        image = image.unsqueeze(0).to(device)  # Add batch dimension\n        \n        # Make prediction\n        model.eval()\n        with torch.no_grad():\n            output = model(image)\n            pred = output.argmax(dim=1).item()\n        \n        # Calculate gradient\n        gradient = compute_gradients(model, image.clone(), target, device)\n        \n        # Display original image\n        axes[i, 0].imshow(image.squeeze().cpu().numpy(), cmap='gray')\n        axes[i, 0].set_title(f'Original (Label: {target})')\n        axes[i, 0].axis('off')\n        \n        # Display gradient\n        axes[i, 1].imshow(gradient, cmap='hot')\n        axes[i, 1].set_title('Feature Importance Heatmap')\n        axes[i, 1].axis('off')\n        \n        # Display overlay\n        img = image.squeeze().cpu().numpy()\n        overlay = np.zeros((28, 28, 3))\n        overlay[:, :, 0] = gradient / gradient.max()  # Red channel for gradient\n        overlay[:, :, 1] = img  # Green channel for original image\n        overlay[:, :, 2] = img  # Blue channel for original image\n        \n        axes[i, 2].imshow(overlay)\n        axes[i, 2].set_title('Overlay Visualization')\n        axes[i, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \nexcept Exception as e:\n    print(f\"Error computing feature importance: {str(e)}\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4242129623889923..2.821486711502075].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4242129623889923..2.821486711502075].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4242129623889923..2.821486711502075].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4242129623889923..2.821486711502075].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4242129623889923..2.821486711502075].\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Model Feature Importance Visualization](model-analysis_files/figure-html/feature-importance-output-2.png){#feature-importance width=1116 height=1910}\n:::\n:::\n\n\n## Summary\n\nFrom the above analysis, we can draw the following conclusions about the MNIST model:\n\n1. The model achieves high classification accuracy, indicating that CNNs are very effective for handwritten digit recognition tasks\n2. Different digits have varying recognition difficulty, with some digits (like 1 and 0) being easier to recognize, while others (like 5 and 8) may be more challenging\n3. The confusion matrix shows the most common confusion pairs (e.g., 4 and 9, or 3 and 5)\n4. Feature importance analysis indicates that the model primarily focuses on structural features of digits, such as strokes and intersections \n\n",
    "supporting": [
      "model-analysis_files"
    ],
    "filters": [],
    "includes": {}
  }
}